{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d094c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import sys\n",
    "\n",
    "sys.path.append('/workspace/notes/metric/pytorchtools.py')\n",
    "# sys.path.remove('/workspace/notes/chukan/Memory/Memory.py')\n",
    "# pprint.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c685ca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "# from torchinfo import summary\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy import signal\n",
    "from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8407edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import slackweb\n",
    "\n",
    "slack = slackweb.Slack(url=\"https://hooks.slack.com/services/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\")\n",
    "slack_from = \"a6000-station\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38ba5f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowpass(x, samplerate):\n",
    "\n",
    "    fp = 40                         # 通過域端周波数[Hz]\n",
    "    fs = 60                         # 阻止域端周波数[Hz]\n",
    "    gpass = 1                       # 通過域最大損失量[dB]\n",
    "    gstop = 40                      # 阻止域最小減衰量[dB]\n",
    "    # 時系列のサンプルデータ作成\n",
    "\n",
    "    n = len(x[:, 0])                         # データ数\n",
    "    dt = 1/samplerate                       # サンプリング間隔\n",
    "    fn = 1/(2*dt)                   # ナイキスト周波数\n",
    "    t = np.linspace(1, n, n)*dt-dt\n",
    "\n",
    "    #print('t=',t)\n",
    "    data_lp = np.array([[0]*3 for i in range(n)], dtype='float32')\n",
    "\n",
    "    # 正規化\n",
    "    Wp = fp/fn\n",
    "    Ws = fs/fn\n",
    "\n",
    "    # ローパスフィルタで波形整形\n",
    "    # バターワースフィルタ\n",
    "    N, Wn = signal.buttord(Wp, Ws, gpass, gstop)\n",
    "    b1, a1 = signal.butter(N, Wn, \"low\")\n",
    "    data_lp[:, 0]= signal.filtfilt(b1, a1, x[:, 0])\n",
    "    data_lp[:, 1]= signal.filtfilt(b1, a1, x[:, 1])\n",
    "    data_lp[:, 2]= signal.filtfilt(b1, a1, x[:, 2])\n",
    "    return data_lp, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8639247c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset():\n",
    "    def __init__(self, root_dir='/workspace/notes/metric/data/6-20', sample_rate=2000, window_size=200, transform=None, del_list=[]):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.del_list = del_list\n",
    "#         self.classes.remove('.ipynb_checkpoints')\n",
    "#         print(self.classes)\n",
    "        self.datas = []\n",
    "        self.labels = []\n",
    "        self.items = []\n",
    "        self.class_list = []\n",
    "        \n",
    "#         haps_csv = pd.read_csv('/workspace/notes/metric/cnn_haps.csv', sep=',', header=None,  encoding=\"utf-8\", error_bad_lines=False)\n",
    "#         haps_tmp = np.array(haps_csv.iloc[:, 0:4],dtype = np.float32 )# 教師データの方の読み取り\n",
    "#         print(haps_tmp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(len(self.del_list)):\n",
    "            del self.classes[self.del_list[i] - i]\n",
    "\n",
    "        for i, class_name in enumerate(self.classes):\n",
    "#             class_dir = os.path.join(root_dir, class_name, \"increasedimgs\")\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            phycho_dir = class_dir + \"/phycho\"\n",
    "            class_dir = class_dir + \"/mems\"\n",
    "            \n",
    "            print(class_name, i)\n",
    "            self.class_list.append(class_name)\n",
    "            \n",
    "            files = os.listdir(class_dir) #触覚センサ信号\n",
    "            phy_file = os.listdir(phycho_dir) #心理実験三次元データ\n",
    "            \n",
    "\n",
    "            try:\n",
    "                files.remove('.ipynb_checkpoints')\n",
    "            except ValueError:\n",
    "#                 print(\"v_e\")\n",
    "                pass\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(class_dir, file_name)\n",
    "                phy_path = os.path.join(phycho_dir, phy_file[0])\n",
    "#                 print(file_path)\n",
    "#                 image = Image.open(image_path)\n",
    "                haps_csv = pd.read_csv(phy_path, sep=',', header=None,  encoding=\"utf-8\", error_bad_lines=False)\n",
    "                haps_tmp = np.array(haps_csv.iloc[:, 0:4],dtype = np.float32 )# 教師データの方の読み取り\n",
    "#                 print(haps_tmp)\n",
    "                \n",
    "\n",
    "                csv = pd.read_csv(file_path, sep=',', header=None, encoding=\"shift-jis\", error_bad_lines=False)\n",
    "    \n",
    "                \n",
    "            \n",
    "#                 tmp = np.array(csv.iloc[ 190000:210000 , 3:6],dtype = np.float32 )\n",
    "                tmp = np.array(csv.iloc[:,3:6],dtype = np.float32 )\n",
    "                \n",
    "                #dataのトリミング\n",
    "                tmp, t = lowpass(tmp, 2000)\n",
    "                filt_G = savgol_filter(tmp, 9, 2, deriv=1, delta=1, axis=0, mode='interp') #微分\n",
    "            \n",
    "#                 max_ege = 0\n",
    "                reach = 250\n",
    "                for danp in range(int(len(filt_G)/reach)):\n",
    "                    win = filt_G[danp*reach:danp*reach+reach, 2]\n",
    "                    sum_d=0\n",
    "                    win = abs(win)\n",
    "                    sum_d = sum(win)/reach\n",
    "                    if(sum_d > 0.0005): #0.0005は目視で確認\n",
    "                        max_ege = danp*reach\n",
    "                        break\n",
    "\n",
    "                min_ege = np.argmin(filt_G[:,2]) #なぞり動作がが終わったところ\n",
    "                \n",
    "                tmp = tmp[max_ege:min_ege]\n",
    "#                 print(max_ege, min_ege)\n",
    "                #tmp = signal.convolve2d(tmp, conv, mode=\"valid\")\n",
    "            \n",
    "                \n",
    "                tmp = tmp.T\n",
    "\n",
    "                tmp = tmp - np.roll(tmp,300)\n",
    "                tmp=tmp[:,sample_rate*2:]\n",
    "#                 print(tmp.shape)\n",
    "\n",
    "#                 for x in range(int(tmp.shape[1]/window_size)): #ノーマル\n",
    "#                     mini_data = tmp[:,x*window_size:x*window_size+window_size]\n",
    "#                 for x in range(int(tmp.shape[1] - window_size)):\n",
    "#                     mini_data = tmp[:,x:x+window_size] #データをカサ増しして取る\n",
    "\n",
    "                for x in range(int((tmp.shape[1]-window_size)//(window_size/2))): #半分かさまし\n",
    "                    mini_data = tmp[:,x*window_size//2:x*window_size//2+window_size]\n",
    "\n",
    "#                     mini_data = mini_data.T\n",
    "                    \n",
    "                    \n",
    "#                     print(mini_data.shape)\n",
    "                    self.datas.append(mini_data) \n",
    "                    self.labels.append(haps_tmp[0, 1:]) #心理実験三次元データ呼び出し\n",
    "                    self.items.append(i) #ラベル\n",
    "#                     print(haps_tmp[0, 1:])\n",
    "        \n",
    "#         print(len(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.datas[index]\n",
    "        label = self.labels[index]\n",
    "        item = self.items[index]\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        \n",
    "        return (data, label, item)\n",
    "    \n",
    "    def __classlist__(self):\n",
    "        return self.class_list\n",
    "\n",
    "# 画像の前処理として、ToTensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e95f17b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gomu 0\n",
      "colk 1\n",
      "wood_hinoki 2\n",
      "arumi 3\n",
      "wasi 4\n",
      "buta-ura 5\n",
      "0\n",
      "['gomu', 'colk', 'wood_hinoki', 'arumi', 'wasi', 'buta-ura']\n"
     ]
    }
   ],
   "source": [
    "# ['danbo', 'gomu', 'colk', 'kanaami', 'wood_hinoki',\n",
    "# 'denim', 'gaze', 'arumi', 'pet', 'sinbun', 'wasi', \n",
    "# 'buta-omote', 'arumi-hoiru', 'jousitu', 'buta-ura', 'koutaku']\n",
    "del_list = [0,3,5,6,8,9,11,12,13,15] #学習に含まないクラス\n",
    "\n",
    "# CustomDatasetクラスをインスタンス化し、データセットオブジェクトを作成する\n",
    "# dataset = CustomDataset(root_dir='/workspace/notes/metric/t_data', transform=transform)\n",
    "dataset = CustomDataset(transform=transform, del_list=del_list)\n",
    "\n",
    "da, ta, it= dataset.__getitem__(0)\n",
    "print(it)\n",
    "# print(torch.squeeze(da,dim=0).shape)\n",
    "# print(ta)\n",
    "# print(len(dataset))\n",
    "print(dataset.__classlist__())\n",
    "\n",
    "# x_data = np.empty((1,80,3))\n",
    "# y_data = np.empty((1,1))\n",
    "\n",
    "# for i in range(len(dataset)):\n",
    "#     da, ta = dataset.__getitem__(i)\n",
    "#     if i==0:\n",
    "#         np.arange(da)\n",
    "#         ndarray(ta)\n",
    "    \n",
    "#     np.append(x_data, da, axis=0)\n",
    "#     np.append(y_data, ta, axis=0)\n",
    "    \n",
    "    \n",
    "# print(x_data.shape)\n",
    "# print(y_data.shape)\n",
    "# print(dataset.datas)\n",
    "# print(dataset.labels)\n",
    "\n",
    "# # DataLoaderを使用して、バッチサイズを指定して、データを読み込んで処理する\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "# print(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b13f7010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データ、検証データに 8:2 の割合で分割する。\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# indices = np.arange(len(dataset))\n",
    "\n",
    "# train_dataset = torch.utils.data.Subset(dataloader, indices[:train_size])\n",
    "# val_dataset = torch.utils.data.Subset(dataloader, indices[train_size:])\n",
    "\n",
    "# print(f\"full: {len(dataset)} -> train: {len(train_dataset)}, test: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5d4b991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full: 2103 -> train: 1682, test: 421\n"
     ]
    }
   ],
   "source": [
    "# 学習データ、検証データに 8:2 の割合で分割する。\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "print(f\"full: {len(dataset)} -> train: {len(train_dataset)}, test: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "172c3afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8718d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93697816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f4636486b50>\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42d89946",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "1 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "2 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "3 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "4 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "5 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "6 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "7 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "8 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "9 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "10 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "11 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "12 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "13 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "14 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "15 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "16 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "17 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "18 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "19 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "20 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "21 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "22 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "23 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "24 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "25 torch.Size([64, 1, 3, 200]) torch.Size([64, 3])\n",
      "26 torch.Size([18, 1, 3, 200]) torch.Size([18, 3])\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=1)#200くらいで十分そう\n",
    "\n",
    "# for j, inp in enumerate(train_loader):\n",
    "#     data, label , rabel_num= inp\n",
    "#     data = torch.squeeze(da,dim=0)\n",
    "    print(j, data.shape,label.shape)\n",
    "# for i in range(len(dataset)):       \n",
    "#     da, ta = dataset.__getitem__(i)\n",
    "#     print(ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "88681984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderを使用して、バッチサイズを指定して、データを読み込んで処理する\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee08eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Model(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(1, 80, 3)\n",
    "#         self.fc2 = nn.Linear(32, 64, 1)\n",
    "#         self.out = nn.Linear(64, 1, 7)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.out(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "258857f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(1)\n",
    "# model = Model()\n",
    "# # 損失関数\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# # 最適化関数\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5eb304e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [16, 3, 3, 3], but got 5-dimensional input of size [2, 64, 3, 1, 200] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-478e1d842030>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1014\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1015\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-b2bd6c629d36>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;31m#         x = self.maxpool(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m             for hook in itertools.chain(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 395\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [16, 3, 3, 3], but got 5-dimensional input of size [2, 64, 3, 1, 200] instead"
     ]
    }
   ],
   "source": [
    "# model = CNN().cuda()\n",
    "# summary(model, (64, 3, 1, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc7b6c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN_1d(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNN_1d, self).__init__()\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.conv1 = nn.Conv1d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "#         self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "#         self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.fc1 = nn.Linear(16 * 20 * 1, 7)\n",
    "\n",
    "#     def forward(self, x):\n",
    "# #         x = self.flatten(x)\n",
    "# #         x = self.conv1(x)\n",
    "#         x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
    "# #         x = self.conv2(x)\n",
    "#         x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
    "#         x = x.view(-1, 16 * 20 * 1)\n",
    "#         x = self.fc1(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dca47e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # モデルの定義\n",
    "# # class CNN(nn.Module):\n",
    "# #     def __init__(self):\n",
    "# #         super(CNN, self).__init__()\n",
    "# #         self.relu = nn.ReLU()\n",
    "# #         self.conv1 = nn.Conv2d(3, 64, kernel_size=1, stride=1, padding=0)\n",
    "# #         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "# # #         self.bn1 = nn.BatchNorm1d(8, track_running_stats=False)\n",
    "# #         self.conv2 = nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1)\n",
    "# # #         self.pool2 = nn.MaxPool2d(kernel_size=1, stride=1)\n",
    "# #         self.fc1 = nn.Linear(32 * 40 * 1, 3)\n",
    "# # #         self.fc2 = nn.Linear(77, 7)\n",
    "\n",
    "# #     def forward(self, x):\n",
    "# #         x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
    "# # #         x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
    "# #         x = nn.functional.relu(self.conv2(x))\n",
    "# # #         x = self.pool2(x)\n",
    "# #         x = x.view(-1, 32 * 40 * 1)\n",
    "# #         x = self.fc1(x)\n",
    "# # #         x = self.relu(x)\n",
    "# # #         x = self.fc2(x)\n",
    "# #         return x\n",
    "\n",
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding =1)\n",
    "#         self.fc = nn.Linear(128 * 100, 3)  # 64: チャンネル数, 100: 畳み込み後の特徴量数\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.conv1(x))\n",
    "# #         x = self.maxpool(x)\n",
    "#         x = self.relu(self.conv2(x))\n",
    "# #         x = self.maxpool(x)\n",
    "#         x = self.relu(self.conv3(x))\n",
    "# #         x = self.maxpool(x)\n",
    "#         x = x.view(x.size(0), -1)  # バッチサイズを維持しながら1次元に変形\n",
    "#         x = self.fc(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4736ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "#         super(LSTM, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "#         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "#         out, _ = self.lstm(x, (h0, c0))\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         return out\n",
    "\n",
    "\n",
    "\n",
    "# input_size = 10  # 入力の次元数\n",
    "# hidden_size = 20  # LSTMの隠れ層の次元数\n",
    "# num_layers = 3  # LSTMの層数\n",
    "# output_size = 1  # 出力の次元数\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_layer_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.rnn1 = nn.RNN(3, \n",
    "                            20, \n",
    "                            num_layers = 1\n",
    "                            )\n",
    "        self.bn1 = nn.BatchNorm1d(1024, track_running_stats=False)\n",
    "        #self.maxpool1 = nn.MaxPool1d(kernel_size=8, stride=4)\n",
    "        \n",
    "        self.rnn2 = nn.RNN(20, \n",
    "                            16, \n",
    "                            num_layers = 1)\n",
    "        self.bn2 = nn.BatchNorm1d(256, track_running_stats=False)\n",
    "        \n",
    "        self.rnn3 = nn.RNN(16, \n",
    "                            12, \n",
    "                            num_layers = 1)\n",
    "        self.bn3 = nn.BatchNorm1d(32, track_running_stats=False)\n",
    "        \n",
    "        self.rnn4 = nn.RNN(12, \n",
    "                            hidden_layer_size, \n",
    "                            num_layers = 1)\n",
    "        #self.maxpool2 = nn.MaxPool1d(kernel_size=2, stride=1)\n",
    "        \n",
    "        self.dense1 = nn.Linear(hidden_layer_size, 1024)\n",
    "        self.bn4 = nn.BatchNorm1d(1024)\n",
    "        \n",
    "        self.dense2 = nn.Linear(1024, 3)\n",
    "        self.fc = nn.Linear(200, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        batch_size = x.shape[0]        \n",
    "        x = x.view(batch_size, 200 ,3)  # (Batch, Cannel, Height, Width) -> (Batch, Height, Width) = (Batch, Seqence, Feature)\n",
    "                                                                 # 画像の Height を時系列のSequenceに、Width を特徴量の次元としてLSTMに入力する\n",
    "        x = x.permute(1, 0, 2)                                   # (Batch, Seqence, Feature) -> (Seqence , Batch, Feature)\n",
    "\n",
    "        en_h1, en_hn1 = self.rnn1(x)\n",
    "        en_h2, en_hn2 = self.rnn2(en_h1)\n",
    "        en_h3, en_hn3 = self.rnn3(en_h2)\n",
    "        en_h4, en_hn4 = self.rnn4(en_h3)\n",
    "        \n",
    "#         hn = en_hn4.squeeze(0)  # テンソルの次元を削減\n",
    "#         print(hn.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        de_h1 = F.leaky_relu(self.bn4(self.dense1(en_h4[-1,:,:])))\n",
    "        output = F.log_softmax(self.dense2(de_h1), dim=1)\n",
    "#         return en_h4[-1,:,:]\n",
    "        return output\n",
    "\n",
    "    \n",
    "#モデルのインスタンス化\n",
    "\n",
    "\n",
    "# # 入力データの作成\n",
    "# input_data = torch.randn(128, 200, 3)  # (バッチサイズ, シーケンス長, 入力の次元数)\n",
    "\n",
    "# # モデルに入力データを渡して予測\n",
    "# model = LSTM(3)\n",
    "# output = model(input_data)\n",
    "\n",
    "# # 出力の形状を表示\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1135ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #resnet\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# class block(nn.Module):\n",
    "#     def __init__(self, first_conv_in_channels, first_conv_out_channels, identity_conv=None, stride=1):\n",
    "#         \"\"\"\n",
    "#         残差ブロックを作成するクラス\n",
    "#         Args:\n",
    "#             first_conv_in_channels : 1番目のconv層（1×1）のinput channel数\n",
    "#             first_conv_out_channels : 1番目のconv層（1×1）のoutput channel数\n",
    "#             identity_conv : channel数調整用のconv層\n",
    "#             stride : 3×3conv層におけるstide数。sizeを半分にしたいときは2に設定\n",
    "#         \"\"\"        \n",
    "#         super(block, self).__init__()\n",
    "\n",
    "#         # 1番目のconv層（1×1）\n",
    "#         self.conv1 = nn.Conv2d(\n",
    "#             first_conv_in_channels, first_conv_out_channels, kernel_size=1, stride=1, padding=0)\n",
    "#         self.bn1 = nn.BatchNorm2d(first_conv_out_channels)\n",
    "\n",
    "#         # 2番目のconv層（3×3）\n",
    "#         # パターン3の時はsizeを変更できるようにstrideは可変\n",
    "#         self.conv2 = nn.Conv2d(\n",
    "#             first_conv_out_channels, first_conv_out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "#         self.bn2 = nn.BatchNorm2d(first_conv_out_channels)\n",
    "\n",
    "#         # 3番目のconv層（1×1）\n",
    "#         # output channelはinput channelの4倍になる\n",
    "#         self.conv3 = nn.Conv2d(\n",
    "#             first_conv_out_channels, first_conv_out_channels*4, kernel_size=1, stride=1, padding=0)\n",
    "#         self.bn3 = nn.BatchNorm2d(first_conv_out_channels*4)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#         # identityのchannel数の調整が必要な場合はconv層（1×1）を用意、不要な場合はNone\n",
    "#         self.identity_conv = identity_conv\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         identity = x.clone()  # 入力を保持する\n",
    "\n",
    "#         x = self.conv1(x)  # 1×1の畳み込み\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.conv2(x)  # 3×3の畳み込み（パターン3の時はstrideが2になるため、ここでsizeが半分になる）\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.conv3(x)  # 1×1の畳み込み\n",
    "#         x = self.bn3(x)\n",
    "\n",
    "#         # 必要な場合はconv層（1×1）を通してidentityのchannel数の調整してから足す\n",
    "#         if self.identity_conv is not None:\n",
    "#             identity = self.identity_conv(identity)\n",
    "#         x += identity\n",
    "\n",
    "#         x = self.relu(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5e0c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResNet(nn.Module):\n",
    "#     def __init__(self, block, layers, num_classes):\n",
    "#         super(ResNet, self).__init__()\n",
    "\n",
    "#         # conv1はアーキテクチャ通りにベタ打ち\n",
    "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "#         # conv2_xはサイズの変更は不要のため、strideは1\n",
    "#         self.conv2_x = self._make_layer(block, layers[0], res_block_in_channels=64, first_conv_out_channels=64, stride=1)\n",
    "\n",
    "#         # conv3_x以降はサイズの変更をする必要があるため、strideは2\n",
    "#         self.conv3_x = self._make_layer(block, layers[1], res_block_in_channels=256,  first_conv_out_channels=128, stride=2)\n",
    "#         self.conv4_x = self._make_layer(block, layers[2], res_block_in_channels=512,  first_conv_out_channels=256, stride=2)\n",
    "#         self.conv5_x = self._make_layer(block, layers[3], res_block_in_channels=1024, first_conv_out_channels=512, stride=2)\n",
    "\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "#         self.fc = nn.Linear(512*4, num_classes)\n",
    "\n",
    "#     def forward(self,x):\n",
    "\n",
    "#         x = self.conv1(x)   # in:(3,224*224)、out:(64,112*112)\n",
    "#         x = self.bn1(x)     # in:(64,112*112)、out:(64,112*112)\n",
    "#         x = self.relu(x)    # in:(64,112*112)、out:(64,112*112)\n",
    "#         x = self.maxpool(x) # in:(64,112*112)、out:(64,56*56)\n",
    "\n",
    "#         x = self.conv2_x(x)  # in:(64,56*56)  、out:(256,56*56)\n",
    "#         x = self.conv3_x(x)  # in:(256,56*56) 、out:(512,28*28)\n",
    "#         x = self.conv4_x(x)  # in:(512,28*28) 、out:(1024,14*14)\n",
    "#         x = self.conv5_x(x)  # in:(1024,14*14)、out:(2048,7*7)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.reshape(x.shape[0], -1)\n",
    "#         x = self.fc(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "#     def _make_layer(self, block, num_res_blocks, res_block_in_channels, first_conv_out_channels, stride):\n",
    "#         layers = []\n",
    "\n",
    "#         # 1つ目の残差ブロックではchannel調整、及びsize調整が発生する\n",
    "#         # identifyを足す前に1×1のconv層を追加し、サイズ調整が必要な場合はstrideを2に設定\n",
    "#         identity_conv = nn.Conv2d(res_block_in_channels, first_conv_out_channels*4, kernel_size=1,stride=stride)\n",
    "#         layers.append(block(res_block_in_channels, first_conv_out_channels, identity_conv, stride))\n",
    "\n",
    "#         # 2つ目以降のinput_channel数は1つ目のoutput_channelの4倍\n",
    "#         in_channels = first_conv_out_channels*4\n",
    "\n",
    "#         # channel調整、size調整は発生しないため、identity_convはNone、strideは1\n",
    "#         for i in range(num_res_blocks - 1):\n",
    "#             layers.append(block(in_channels, first_conv_out_channels, identity_conv=None, stride=1))\n",
    "\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "    \n",
    "# def ResNet18(block, num_classes):\n",
    "#   return ResNet(block, [2,2,2,2], num_classes)\n",
    "\n",
    "# def ResNet50(block, num_classes):\n",
    "#   return ResNet(block, [3,4,6,3], num_classes)\n",
    "\n",
    "# def ResNet101(block, num_classes):\n",
    "#   return ResNet(block, [3,4,23,3], num_classes)\n",
    "\n",
    "# def ResNet152(block, num_classes):\n",
    "#   return ResNet(block, [3,8,36,3], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb4a59c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb37515b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torchvision.models as models\n",
    "\n",
    "# # ResNet-50モデルを定義\n",
    "# # model = models.resnet50(pretrained=False)\n",
    "# # model = models.resnet101(pretrained=False)\n",
    "# # 最終の全結合層を入力サイズに合わせて調整\n",
    "# # num_features = model.fc.in_features\n",
    "# # model.fc = nn.Linear(num_features, 3)  # 10は分類するクラス数に合わせて調整\n",
    "\n",
    "# # 入力データのサイズを定義\n",
    "# input_size = (3, 80, 80)\n",
    "\n",
    "# # ダミーの入力データを生成\n",
    "# dummy_input = torch.randn(1, *input_size)\n",
    "\n",
    "# # モデルにダミーの入力データを入力し、出力のサイズを確認\n",
    "# # output = model(dummy_input)\n",
    "# # print(\"Output shape:\", output.shape)\n",
    "# # model = CNN().cuda()\n",
    "# model = ResNet18(block,3).cuda()\n",
    "# summary(model, (64, 3, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "473ebcc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] train_loss: 0.543\n",
      "[1] val_loss: 0.639\n",
      "[2] train_loss: 0.565\n",
      "[2] val_loss: 0.638\n",
      "[3] train_loss: 0.538\n",
      "[3] val_loss: 0.638\n",
      "[4] train_loss: 0.541\n",
      "[4] val_loss: 0.638\n",
      "[5] train_loss: 0.536\n",
      "[5] val_loss: 0.638\n",
      "[6] train_loss: 0.534\n",
      "[6] val_loss: 0.638\n",
      "[7] train_loss: 0.531\n",
      "[7] val_loss: 0.638\n",
      "[8] train_loss: 0.537\n",
      "[8] val_loss: 0.638\n",
      "[9] train_loss: 0.528\n",
      "[9] val_loss: 0.638\n",
      "[10] train_loss: 0.534\n",
      "[10] val_loss: 0.638\n",
      "[11] train_loss: 0.539\n",
      "[11] val_loss: 0.638\n",
      "[12] train_loss: 0.538\n",
      "[12] val_loss: 0.638\n",
      "[13] train_loss: 0.542\n",
      "[13] val_loss: 0.638\n",
      "[14] train_loss: 0.530\n",
      "[14] val_loss: 0.638\n",
      "[15] train_loss: 0.537\n",
      "[15] val_loss: 0.638\n",
      "[16] train_loss: 0.536\n",
      "[16] val_loss: 0.638\n",
      "[17] train_loss: 0.542\n",
      "[17] val_loss: 0.638\n",
      "[18] train_loss: 0.537\n",
      "[18] val_loss: 0.638\n",
      "[19] train_loss: 0.538\n",
      "[19] val_loss: 0.638\n",
      "[20] train_loss: 0.551\n",
      "[20] val_loss: 0.638\n",
      "[21] train_loss: 0.536\n",
      "[21] val_loss: 0.638\n",
      "[22] train_loss: 0.538\n",
      "[22] val_loss: 0.638\n",
      "[23] train_loss: 0.548\n",
      "[23] val_loss: 0.638\n",
      "[24] train_loss: 0.545\n",
      "[24] val_loss: 0.638\n",
      "[25] train_loss: 0.542\n",
      "[25] val_loss: 0.638\n",
      "[26] train_loss: 0.540\n",
      "[26] val_loss: 0.638\n",
      "[27] train_loss: 0.542\n",
      "[27] val_loss: 0.638\n",
      "[28] train_loss: 0.545\n",
      "[28] val_loss: 0.638\n",
      "[29] train_loss: 0.531\n",
      "[29] val_loss: 0.638\n",
      "[30] train_loss: 0.534\n",
      "[30] val_loss: 0.638\n",
      "[31] train_loss: 0.539\n",
      "[31] val_loss: 0.638\n",
      "[32] train_loss: 0.531\n",
      "[32] val_loss: 0.638\n",
      "[33] train_loss: 0.538\n",
      "[33] val_loss: 0.638\n",
      "[34] train_loss: 0.537\n",
      "[34] val_loss: 0.638\n",
      "[35] train_loss: 0.546\n",
      "[35] val_loss: 0.638\n",
      "[36] train_loss: 0.543\n",
      "[36] val_loss: 0.638\n",
      "[37] train_loss: 0.548\n",
      "[37] val_loss: 0.638\n",
      "[38] train_loss: 0.535\n",
      "[38] val_loss: 0.638\n",
      "[39] train_loss: 0.539\n",
      "[39] val_loss: 0.638\n",
      "[40] train_loss: 0.534\n",
      "[40] val_loss: 0.638\n",
      "[41] train_loss: 0.549\n",
      "[41] val_loss: 0.638\n",
      "[42] train_loss: 0.538\n",
      "[42] val_loss: 0.638\n",
      "[43] train_loss: 0.549\n",
      "[43] val_loss: 0.638\n",
      "[44] train_loss: 0.528\n",
      "[44] val_loss: 0.638\n",
      "[45] train_loss: 0.532\n",
      "[45] val_loss: 0.638\n",
      "[46] train_loss: 0.549\n",
      "[46] val_loss: 0.638\n",
      "[47] train_loss: 0.545\n",
      "[47] val_loss: 0.638\n",
      "[48] train_loss: 0.539\n",
      "[48] val_loss: 0.638\n",
      "[49] train_loss: 0.540\n",
      "[49] val_loss: 0.638\n",
      "[50] train_loss: 0.534\n",
      "[50] val_loss: 0.638\n",
      "[51] train_loss: 0.549\n",
      "[51] val_loss: 0.638\n",
      "[52] train_loss: 0.544\n",
      "[52] val_loss: 0.638\n",
      "[53] train_loss: 0.532\n",
      "[53] val_loss: 0.638\n",
      "[54] train_loss: 0.542\n",
      "[54] val_loss: 0.638\n",
      "[55] train_loss: 0.544\n",
      "[55] val_loss: 0.638\n",
      "[56] train_loss: 0.541\n",
      "[56] val_loss: 0.638\n",
      "[57] train_loss: 0.539\n",
      "[57] val_loss: 0.638\n",
      "[58] train_loss: 0.541\n",
      "[58] val_loss: 0.638\n",
      "[59] train_loss: 0.538\n",
      "[59] val_loss: 0.638\n",
      "[60] train_loss: 0.541\n",
      "[60] val_loss: 0.638\n",
      "[61] train_loss: 0.541\n",
      "[61] val_loss: 0.638\n",
      "[62] train_loss: 0.536\n",
      "[62] val_loss: 0.638\n",
      "[63] train_loss: 0.542\n",
      "[63] val_loss: 0.638\n",
      "[64] train_loss: 0.542\n",
      "[64] val_loss: 0.638\n",
      "[65] train_loss: 0.541\n",
      "[65] val_loss: 0.638\n",
      "[66] train_loss: 0.548\n",
      "[66] val_loss: 0.638\n",
      "[67] train_loss: 0.529\n",
      "[67] val_loss: 0.638\n",
      "[68] train_loss: 0.533\n",
      "[68] val_loss: 0.638\n",
      "[69] train_loss: 0.532\n",
      "[69] val_loss: 0.638\n",
      "[70] train_loss: 0.537\n",
      "[70] val_loss: 0.638\n",
      "[71] train_loss: 0.550\n",
      "[71] val_loss: 0.638\n",
      "[72] train_loss: 0.535\n",
      "[72] val_loss: 0.638\n",
      "[73] train_loss: 0.541\n",
      "[73] val_loss: 0.638\n",
      "[74] train_loss: 0.541\n",
      "[74] val_loss: 0.638\n",
      "[75] train_loss: 0.547\n",
      "[75] val_loss: 0.638\n",
      "[76] train_loss: 0.528\n",
      "[76] val_loss: 0.638\n",
      "[77] train_loss: 0.534\n",
      "[77] val_loss: 0.638\n",
      "[78] train_loss: 0.537\n",
      "[78] val_loss: 0.638\n",
      "[79] train_loss: 0.534\n",
      "[79] val_loss: 0.638\n",
      "[80] train_loss: 0.546\n",
      "[80] val_loss: 0.638\n",
      "[81] train_loss: 0.542\n",
      "[81] val_loss: 0.638\n",
      "[82] train_loss: 0.530\n",
      "[82] val_loss: 0.638\n",
      "[83] train_loss: 0.534\n",
      "[83] val_loss: 0.638\n",
      "[84] train_loss: 0.533\n",
      "[84] val_loss: 0.638\n",
      "[85] train_loss: 0.548\n",
      "[85] val_loss: 0.638\n",
      "[86] train_loss: 0.551\n",
      "[86] val_loss: 0.638\n",
      "[87] train_loss: 0.541\n",
      "[87] val_loss: 0.638\n",
      "[88] train_loss: 0.532\n",
      "[88] val_loss: 0.638\n",
      "[89] train_loss: 0.542\n",
      "[89] val_loss: 0.638\n",
      "[90] train_loss: 0.546\n",
      "[90] val_loss: 0.638\n",
      "[91] train_loss: 0.530\n",
      "[91] val_loss: 0.638\n",
      "[92] train_loss: 0.539\n",
      "[92] val_loss: 0.638\n",
      "[93] train_loss: 0.531\n",
      "[93] val_loss: 0.638\n",
      "[94] train_loss: 0.524\n",
      "[94] val_loss: 0.638\n",
      "[95] train_loss: 0.538\n",
      "[95] val_loss: 0.638\n",
      "[96] train_loss: 0.534\n",
      "[96] val_loss: 0.638\n",
      "[97] train_loss: 0.544\n",
      "[97] val_loss: 0.638\n",
      "[98] train_loss: 0.546\n",
      "[98] val_loss: 0.638\n",
      "[99] train_loss: 0.538\n",
      "[99] val_loss: 0.638\n",
      "[100] train_loss: 0.542\n",
      "[100] val_loss: 0.638\n",
      "[101] train_loss: 0.542\n",
      "[101] val_loss: 0.638\n",
      "[102] train_loss: 0.527\n",
      "[102] val_loss: 0.638\n",
      "[103] train_loss: 0.553\n",
      "[103] val_loss: 0.638\n",
      "[104] train_loss: 0.540\n",
      "[104] val_loss: 0.638\n",
      "[105] train_loss: 0.547\n",
      "[105] val_loss: 0.638\n",
      "[106] train_loss: 0.534\n",
      "[106] val_loss: 0.638\n",
      "[107] train_loss: 0.535\n",
      "[107] val_loss: 0.638\n",
      "[108] train_loss: 0.533\n",
      "[108] val_loss: 0.638\n",
      "[109] train_loss: 0.538\n",
      "[109] val_loss: 0.638\n",
      "[110] train_loss: 0.546\n",
      "[110] val_loss: 0.638\n",
      "[111] train_loss: 0.544\n",
      "[111] val_loss: 0.638\n",
      "[112] train_loss: 0.557\n",
      "[112] val_loss: 0.638\n",
      "[113] train_loss: 0.535\n",
      "[113] val_loss: 0.638\n",
      "[114] train_loss: 0.545\n",
      "[114] val_loss: 0.638\n",
      "[115] train_loss: 0.553\n",
      "[115] val_loss: 0.638\n",
      "[116] train_loss: 0.534\n",
      "[116] val_loss: 0.638\n",
      "[117] train_loss: 0.546\n",
      "[117] val_loss: 0.638\n",
      "[118] train_loss: 0.541\n",
      "[118] val_loss: 0.638\n",
      "[119] train_loss: 0.542\n",
      "[119] val_loss: 0.638\n",
      "[120] train_loss: 0.543\n",
      "[120] val_loss: 0.638\n",
      "[121] train_loss: 0.529\n",
      "[121] val_loss: 0.638\n",
      "[122] train_loss: 0.539\n",
      "[122] val_loss: 0.638\n",
      "[123] train_loss: 0.548\n",
      "[123] val_loss: 0.638\n",
      "[124] train_loss: 0.541\n",
      "[124] val_loss: 0.638\n",
      "[125] train_loss: 0.535\n",
      "[125] val_loss: 0.638\n",
      "[126] train_loss: 0.547\n",
      "[126] val_loss: 0.638\n",
      "[127] train_loss: 0.541\n",
      "[127] val_loss: 0.638\n",
      "[128] train_loss: 0.537\n",
      "[128] val_loss: 0.638\n",
      "[129] train_loss: 0.544\n",
      "[129] val_loss: 0.638\n",
      "[130] train_loss: 0.539\n",
      "[130] val_loss: 0.638\n",
      "[131] train_loss: 0.542\n",
      "[131] val_loss: 0.638\n",
      "[132] train_loss: 0.541\n",
      "[132] val_loss: 0.638\n",
      "[133] train_loss: 0.537\n",
      "[133] val_loss: 0.638\n",
      "[134] train_loss: 0.540\n",
      "[134] val_loss: 0.638\n",
      "[135] train_loss: 0.546\n",
      "[135] val_loss: 0.638\n",
      "[136] train_loss: 0.548\n",
      "[136] val_loss: 0.638\n",
      "[137] train_loss: 0.533\n",
      "[137] val_loss: 0.638\n",
      "[138] train_loss: 0.542\n",
      "[138] val_loss: 0.638\n",
      "[139] train_loss: 0.527\n",
      "[139] val_loss: 0.638\n",
      "[140] train_loss: 0.538\n",
      "[140] val_loss: 0.638\n",
      "[141] train_loss: 0.532\n",
      "[141] val_loss: 0.638\n",
      "[142] train_loss: 0.543\n",
      "[142] val_loss: 0.638\n",
      "[143] train_loss: 0.537\n",
      "[143] val_loss: 0.638\n",
      "[144] train_loss: 0.545\n",
      "[144] val_loss: 0.638\n",
      "[145] train_loss: 0.532\n",
      "[145] val_loss: 0.638\n",
      "[146] train_loss: 0.536\n",
      "[146] val_loss: 0.638\n",
      "[147] train_loss: 0.553\n",
      "[147] val_loss: 0.638\n",
      "[148] train_loss: 0.547\n",
      "[148] val_loss: 0.638\n",
      "[149] train_loss: 0.540\n",
      "[149] val_loss: 0.638\n",
      "[150] train_loss: 0.549\n",
      "[150] val_loss: 0.638\n",
      "[151] train_loss: 0.550\n",
      "[151] val_loss: 0.638\n",
      "[152] train_loss: 0.545\n",
      "[152] val_loss: 0.638\n",
      "[153] train_loss: 0.533\n",
      "[153] val_loss: 0.638\n",
      "[154] train_loss: 0.552\n",
      "[154] val_loss: 0.638\n",
      "[155] train_loss: 0.521\n",
      "[155] val_loss: 0.638\n",
      "[156] train_loss: 0.548\n",
      "[156] val_loss: 0.638\n",
      "[157] train_loss: 0.547\n",
      "[157] val_loss: 0.638\n",
      "[158] train_loss: 0.531\n",
      "[158] val_loss: 0.638\n",
      "[159] train_loss: 0.539\n",
      "[159] val_loss: 0.638\n",
      "[160] train_loss: 0.538\n",
      "[160] val_loss: 0.638\n",
      "[161] train_loss: 0.537\n",
      "[161] val_loss: 0.638\n",
      "[162] train_loss: 0.547\n",
      "[162] val_loss: 0.638\n",
      "[163] train_loss: 0.530\n",
      "[163] val_loss: 0.638\n",
      "[164] train_loss: 0.541\n",
      "[164] val_loss: 0.638\n",
      "[165] train_loss: 0.535\n",
      "[165] val_loss: 0.638\n",
      "[166] train_loss: 0.535\n",
      "[166] val_loss: 0.638\n",
      "[167] train_loss: 0.549\n",
      "[167] val_loss: 0.638\n",
      "[168] train_loss: 0.542\n",
      "[168] val_loss: 0.638\n",
      "[169] train_loss: 0.562\n",
      "[169] val_loss: 0.638\n",
      "[170] train_loss: 0.535\n",
      "[170] val_loss: 0.638\n",
      "[171] train_loss: 0.542\n",
      "[171] val_loss: 0.638\n",
      "[172] train_loss: 0.545\n",
      "[172] val_loss: 0.638\n",
      "[173] train_loss: 0.534\n",
      "[173] val_loss: 0.638\n",
      "[174] train_loss: 0.552\n",
      "[174] val_loss: 0.638\n",
      "[175] train_loss: 0.553\n",
      "[175] val_loss: 0.638\n",
      "[176] train_loss: 0.536\n",
      "[176] val_loss: 0.638\n",
      "[177] train_loss: 0.552\n",
      "[177] val_loss: 0.638\n",
      "[178] train_loss: 0.531\n",
      "[178] val_loss: 0.638\n",
      "[179] train_loss: 0.553\n",
      "[179] val_loss: 0.638\n",
      "[180] train_loss: 0.546\n",
      "[180] val_loss: 0.638\n",
      "[181] train_loss: 0.549\n",
      "[181] val_loss: 0.638\n",
      "[182] train_loss: 0.552\n",
      "[182] val_loss: 0.638\n",
      "[183] train_loss: 0.526\n",
      "[183] val_loss: 0.638\n",
      "[184] train_loss: 0.542\n",
      "[184] val_loss: 0.638\n",
      "[185] train_loss: 0.549\n",
      "[185] val_loss: 0.638\n",
      "[186] train_loss: 0.541\n",
      "[186] val_loss: 0.638\n",
      "[187] train_loss: 0.548\n",
      "[187] val_loss: 0.638\n",
      "[188] train_loss: 0.541\n",
      "[188] val_loss: 0.638\n",
      "[189] train_loss: 0.538\n",
      "[189] val_loss: 0.638\n",
      "[190] train_loss: 0.545\n",
      "[190] val_loss: 0.638\n",
      "[191] train_loss: 0.539\n",
      "[191] val_loss: 0.638\n",
      "[192] train_loss: 0.544\n",
      "[192] val_loss: 0.638\n",
      "[193] train_loss: 0.543\n",
      "[193] val_loss: 0.638\n",
      "[194] train_loss: 0.549\n",
      "[194] val_loss: 0.638\n",
      "[195] train_loss: 0.539\n",
      "[195] val_loss: 0.638\n",
      "[196] train_loss: 0.545\n",
      "[196] val_loss: 0.638\n",
      "[197] train_loss: 0.545\n",
      "[197] val_loss: 0.638\n",
      "[198] train_loss: 0.529\n",
      "[198] val_loss: 0.638\n",
      "[199] train_loss: 0.550\n",
      "[199] val_loss: 0.638\n",
      "[200] train_loss: 0.549\n",
      "[200] val_loss: 0.638\n",
      "[201] train_loss: 0.539\n",
      "[201] val_loss: 0.638\n",
      "[202] train_loss: 0.538\n",
      "[202] val_loss: 0.638\n",
      "[203] train_loss: 0.540\n",
      "[203] val_loss: 0.638\n",
      "[204] train_loss: 0.553\n",
      "[204] val_loss: 0.638\n",
      "[205] train_loss: 0.542\n",
      "[205] val_loss: 0.638\n",
      "[206] train_loss: 0.540\n",
      "[206] val_loss: 0.638\n",
      "[207] train_loss: 0.538\n",
      "[207] val_loss: 0.638\n",
      "[208] train_loss: 0.543\n",
      "[208] val_loss: 0.638\n",
      "[209] train_loss: 0.522\n",
      "[209] val_loss: 0.638\n",
      "[210] train_loss: 0.537\n",
      "[210] val_loss: 0.638\n",
      "[211] train_loss: 0.543\n",
      "[211] val_loss: 0.638\n",
      "[212] train_loss: 0.551\n",
      "[212] val_loss: 0.638\n",
      "[213] train_loss: 0.544\n",
      "[213] val_loss: 0.638\n",
      "[214] train_loss: 0.527\n",
      "[214] val_loss: 0.638\n",
      "[215] train_loss: 0.545\n",
      "[215] val_loss: 0.638\n",
      "[216] train_loss: 0.546\n",
      "[216] val_loss: 0.638\n",
      "[217] train_loss: 0.545\n",
      "[217] val_loss: 0.638\n",
      "[218] train_loss: 0.534\n",
      "[218] val_loss: 0.638\n",
      "[219] train_loss: 0.552\n",
      "[219] val_loss: 0.638\n",
      "[220] train_loss: 0.542\n",
      "[220] val_loss: 0.638\n",
      "[221] train_loss: 0.549\n",
      "[221] val_loss: 0.638\n",
      "[222] train_loss: 0.549\n",
      "[222] val_loss: 0.638\n",
      "[223] train_loss: 0.557\n",
      "[223] val_loss: 0.638\n",
      "[224] train_loss: 0.541\n",
      "[224] val_loss: 0.638\n",
      "[225] train_loss: 0.546\n",
      "[225] val_loss: 0.638\n",
      "[226] train_loss: 0.555\n",
      "[226] val_loss: 0.638\n",
      "[227] train_loss: 0.556\n",
      "[227] val_loss: 0.638\n",
      "[228] train_loss: 0.534\n",
      "[228] val_loss: 0.638\n",
      "[229] train_loss: 0.536\n",
      "[229] val_loss: 0.638\n",
      "[230] train_loss: 0.543\n",
      "[230] val_loss: 0.638\n",
      "[231] train_loss: 0.538\n",
      "[231] val_loss: 0.638\n",
      "[232] train_loss: 0.539\n",
      "[232] val_loss: 0.638\n",
      "[233] train_loss: 0.547\n",
      "[233] val_loss: 0.638\n",
      "[234] train_loss: 0.543\n",
      "[234] val_loss: 0.638\n",
      "[235] train_loss: 0.534\n",
      "[235] val_loss: 0.638\n",
      "[236] train_loss: 0.539\n",
      "[236] val_loss: 0.638\n",
      "[237] train_loss: 0.523\n",
      "[237] val_loss: 0.638\n",
      "[238] train_loss: 0.537\n",
      "[238] val_loss: 0.638\n",
      "[239] train_loss: 0.532\n",
      "[239] val_loss: 0.638\n",
      "[240] train_loss: 0.556\n",
      "[240] val_loss: 0.638\n",
      "[241] train_loss: 0.535\n",
      "[241] val_loss: 0.638\n",
      "[242] train_loss: 0.532\n",
      "[242] val_loss: 0.638\n",
      "[243] train_loss: 0.536\n",
      "[243] val_loss: 0.638\n",
      "[244] train_loss: 0.546\n",
      "[244] val_loss: 0.638\n",
      "[245] train_loss: 0.545\n",
      "[245] val_loss: 0.638\n",
      "[246] train_loss: 0.540\n",
      "[246] val_loss: 0.638\n",
      "[247] train_loss: 0.554\n",
      "[247] val_loss: 0.638\n",
      "[248] train_loss: 0.530\n",
      "[248] val_loss: 0.638\n",
      "[249] train_loss: 0.541\n",
      "[249] val_loss: 0.638\n",
      "[250] train_loss: 0.539\n",
      "[250] val_loss: 0.638\n",
      "[251] train_loss: 0.538\n",
      "[251] val_loss: 0.638\n",
      "[252] train_loss: 0.530\n",
      "[252] val_loss: 0.638\n",
      "[253] train_loss: 0.530\n",
      "[253] val_loss: 0.638\n",
      "[254] train_loss: 0.546\n",
      "[254] val_loss: 0.638\n",
      "[255] train_loss: 0.535\n",
      "[255] val_loss: 0.638\n",
      "[256] train_loss: 0.535\n",
      "[256] val_loss: 0.638\n",
      "[257] train_loss: 0.547\n",
      "[257] val_loss: 0.638\n",
      "[258] train_loss: 0.542\n",
      "[258] val_loss: 0.638\n",
      "[259] train_loss: 0.525\n",
      "[259] val_loss: 0.638\n",
      "[260] train_loss: 0.539\n",
      "[260] val_loss: 0.638\n",
      "[261] train_loss: 0.531\n",
      "[261] val_loss: 0.638\n",
      "[262] train_loss: 0.541\n",
      "[262] val_loss: 0.638\n",
      "[263] train_loss: 0.549\n",
      "[263] val_loss: 0.638\n",
      "[264] train_loss: 0.534\n",
      "[264] val_loss: 0.638\n",
      "[265] train_loss: 0.544\n",
      "[265] val_loss: 0.638\n",
      "[266] train_loss: 0.540\n",
      "[266] val_loss: 0.638\n",
      "[267] train_loss: 0.540\n",
      "[267] val_loss: 0.638\n",
      "[268] train_loss: 0.541\n",
      "[268] val_loss: 0.638\n",
      "[269] train_loss: 0.522\n",
      "[269] val_loss: 0.638\n",
      "[270] train_loss: 0.546\n",
      "[270] val_loss: 0.638\n",
      "[271] train_loss: 0.536\n",
      "[271] val_loss: 0.638\n",
      "[272] train_loss: 0.541\n",
      "[272] val_loss: 0.638\n",
      "[273] train_loss: 0.542\n",
      "[273] val_loss: 0.638\n",
      "[274] train_loss: 0.548\n",
      "[274] val_loss: 0.638\n",
      "[275] train_loss: 0.546\n",
      "[275] val_loss: 0.638\n",
      "[276] train_loss: 0.556\n",
      "[276] val_loss: 0.638\n",
      "[277] train_loss: 0.541\n",
      "[277] val_loss: 0.638\n",
      "[278] train_loss: 0.550\n",
      "[278] val_loss: 0.638\n",
      "[279] train_loss: 0.540\n",
      "[279] val_loss: 0.638\n",
      "[280] train_loss: 0.533\n",
      "[280] val_loss: 0.638\n",
      "[281] train_loss: 0.532\n",
      "[281] val_loss: 0.638\n",
      "[282] train_loss: 0.546\n",
      "[282] val_loss: 0.638\n",
      "[283] train_loss: 0.548\n",
      "[283] val_loss: 0.638\n",
      "[284] train_loss: 0.544\n",
      "[284] val_loss: 0.638\n",
      "[285] train_loss: 0.552\n",
      "[285] val_loss: 0.638\n",
      "[286] train_loss: 0.535\n",
      "[286] val_loss: 0.638\n",
      "[287] train_loss: 0.554\n",
      "[287] val_loss: 0.638\n",
      "[288] train_loss: 0.537\n",
      "[288] val_loss: 0.638\n",
      "[289] train_loss: 0.544\n",
      "[289] val_loss: 0.638\n",
      "[290] train_loss: 0.537\n",
      "[290] val_loss: 0.638\n",
      "[291] train_loss: 0.535\n",
      "[291] val_loss: 0.638\n",
      "[292] train_loss: 0.552\n",
      "[292] val_loss: 0.638\n",
      "[293] train_loss: 0.530\n",
      "[293] val_loss: 0.638\n",
      "[294] train_loss: 0.534\n",
      "[294] val_loss: 0.638\n",
      "[295] train_loss: 0.542\n",
      "[295] val_loss: 0.638\n",
      "[296] train_loss: 0.538\n",
      "[296] val_loss: 0.638\n",
      "[297] train_loss: 0.541\n",
      "[297] val_loss: 0.638\n",
      "[298] train_loss: 0.546\n",
      "[298] val_loss: 0.638\n",
      "[299] train_loss: 0.547\n",
      "[299] val_loss: 0.638\n",
      "[300] train_loss: 0.542\n",
      "[300] val_loss: 0.638\n",
      "[301] train_loss: 0.550\n",
      "[301] val_loss: 0.638\n",
      "[302] train_loss: 0.553\n",
      "[302] val_loss: 0.638\n",
      "[303] train_loss: 0.549\n",
      "[303] val_loss: 0.638\n",
      "[304] train_loss: 0.538\n",
      "[304] val_loss: 0.638\n",
      "[305] train_loss: 0.546\n",
      "[305] val_loss: 0.638\n",
      "[306] train_loss: 0.541\n",
      "[306] val_loss: 0.638\n",
      "[307] train_loss: 0.544\n",
      "[307] val_loss: 0.638\n",
      "[308] train_loss: 0.538\n",
      "[308] val_loss: 0.638\n",
      "[309] train_loss: 0.540\n",
      "[309] val_loss: 0.638\n",
      "[310] train_loss: 0.541\n",
      "[310] val_loss: 0.638\n",
      "[311] train_loss: 0.544\n",
      "[311] val_loss: 0.638\n",
      "[312] train_loss: 0.552\n",
      "[312] val_loss: 0.638\n",
      "[313] train_loss: 0.539\n",
      "[313] val_loss: 0.638\n",
      "[314] train_loss: 0.537\n",
      "[314] val_loss: 0.638\n",
      "[315] train_loss: 0.531\n",
      "[315] val_loss: 0.638\n",
      "[316] train_loss: 0.532\n",
      "[316] val_loss: 0.638\n",
      "[317] train_loss: 0.534\n",
      "[317] val_loss: 0.638\n",
      "[318] train_loss: 0.546\n",
      "[318] val_loss: 0.638\n",
      "[319] train_loss: 0.544\n",
      "[319] val_loss: 0.638\n",
      "[320] train_loss: 0.545\n",
      "[320] val_loss: 0.638\n",
      "[321] train_loss: 0.540\n",
      "[321] val_loss: 0.638\n",
      "[322] train_loss: 0.545\n",
      "[322] val_loss: 0.638\n",
      "[323] train_loss: 0.549\n",
      "[323] val_loss: 0.638\n",
      "[324] train_loss: 0.530\n",
      "[324] val_loss: 0.638\n",
      "[325] train_loss: 0.541\n",
      "[325] val_loss: 0.638\n",
      "[326] train_loss: 0.558\n",
      "[326] val_loss: 0.638\n",
      "[327] train_loss: 0.540\n",
      "[327] val_loss: 0.638\n",
      "[328] train_loss: 0.537\n",
      "[328] val_loss: 0.638\n",
      "[329] train_loss: 0.537\n",
      "[329] val_loss: 0.638\n",
      "[330] train_loss: 0.549\n",
      "[330] val_loss: 0.638\n",
      "[331] train_loss: 0.546\n",
      "[331] val_loss: 0.638\n",
      "[332] train_loss: 0.538\n",
      "[332] val_loss: 0.638\n",
      "[333] train_loss: 0.551\n",
      "[333] val_loss: 0.638\n",
      "[334] train_loss: 0.545\n",
      "[334] val_loss: 0.638\n",
      "[335] train_loss: 0.541\n",
      "[335] val_loss: 0.638\n",
      "[336] train_loss: 0.545\n",
      "[336] val_loss: 0.638\n",
      "[337] train_loss: 0.544\n",
      "[337] val_loss: 0.638\n",
      "[338] train_loss: 0.542\n",
      "[338] val_loss: 0.638\n",
      "[339] train_loss: 0.534\n",
      "[339] val_loss: 0.638\n",
      "[340] train_loss: 0.547\n",
      "[340] val_loss: 0.638\n",
      "[341] train_loss: 0.551\n",
      "[341] val_loss: 0.638\n",
      "[342] train_loss: 0.526\n",
      "[342] val_loss: 0.638\n",
      "[343] train_loss: 0.541\n",
      "[343] val_loss: 0.638\n",
      "[344] train_loss: 0.537\n",
      "[344] val_loss: 0.638\n",
      "[345] train_loss: 0.541\n",
      "[345] val_loss: 0.638\n",
      "[346] train_loss: 0.530\n",
      "[346] val_loss: 0.638\n",
      "[347] train_loss: 0.533\n",
      "[347] val_loss: 0.638\n",
      "[348] train_loss: 0.530\n",
      "[348] val_loss: 0.638\n",
      "[349] train_loss: 0.535\n",
      "[349] val_loss: 0.638\n",
      "[350] train_loss: 0.540\n",
      "[350] val_loss: 0.638\n",
      "[351] train_loss: 0.545\n",
      "[351] val_loss: 0.638\n",
      "[352] train_loss: 0.545\n",
      "[352] val_loss: 0.638\n",
      "[353] train_loss: 0.539\n",
      "[353] val_loss: 0.638\n",
      "[354] train_loss: 0.553\n",
      "[354] val_loss: 0.638\n",
      "[355] train_loss: 0.550\n",
      "[355] val_loss: 0.638\n",
      "[356] train_loss: 0.538\n",
      "[356] val_loss: 0.638\n",
      "[357] train_loss: 0.537\n",
      "[357] val_loss: 0.638\n",
      "[358] train_loss: 0.542\n",
      "[358] val_loss: 0.638\n",
      "[359] train_loss: 0.527\n",
      "[359] val_loss: 0.638\n",
      "[360] train_loss: 0.562\n",
      "[360] val_loss: 0.638\n",
      "[361] train_loss: 0.553\n",
      "[361] val_loss: 0.638\n",
      "[362] train_loss: 0.535\n",
      "[362] val_loss: 0.638\n",
      "[363] train_loss: 0.548\n",
      "[363] val_loss: 0.638\n",
      "[364] train_loss: 0.540\n",
      "[364] val_loss: 0.638\n",
      "[365] train_loss: 0.541\n",
      "[365] val_loss: 0.638\n",
      "[366] train_loss: 0.537\n",
      "[366] val_loss: 0.638\n",
      "[367] train_loss: 0.535\n",
      "[367] val_loss: 0.638\n",
      "[368] train_loss: 0.542\n",
      "[368] val_loss: 0.638\n",
      "[369] train_loss: 0.542\n",
      "[369] val_loss: 0.638\n",
      "[370] train_loss: 0.559\n",
      "[370] val_loss: 0.638\n",
      "[371] train_loss: 0.544\n",
      "[371] val_loss: 0.638\n",
      "[372] train_loss: 0.547\n",
      "[372] val_loss: 0.638\n",
      "[373] train_loss: 0.560\n",
      "[373] val_loss: 0.638\n",
      "[374] train_loss: 0.538\n",
      "[374] val_loss: 0.638\n",
      "[375] train_loss: 0.529\n",
      "[375] val_loss: 0.638\n",
      "[376] train_loss: 0.529\n",
      "[376] val_loss: 0.638\n",
      "[377] train_loss: 0.553\n",
      "[377] val_loss: 0.638\n",
      "[378] train_loss: 0.535\n",
      "[378] val_loss: 0.638\n",
      "[379] train_loss: 0.552\n",
      "[379] val_loss: 0.638\n",
      "[380] train_loss: 0.552\n",
      "[380] val_loss: 0.638\n",
      "[381] train_loss: 0.528\n",
      "[381] val_loss: 0.638\n",
      "[382] train_loss: 0.540\n",
      "[382] val_loss: 0.638\n",
      "[383] train_loss: 0.539\n",
      "[383] val_loss: 0.638\n",
      "[384] train_loss: 0.544\n",
      "[384] val_loss: 0.638\n",
      "[385] train_loss: 0.542\n",
      "[385] val_loss: 0.638\n",
      "[386] train_loss: 0.545\n",
      "[386] val_loss: 0.638\n",
      "[387] train_loss: 0.540\n",
      "[387] val_loss: 0.638\n",
      "[388] train_loss: 0.538\n",
      "[388] val_loss: 0.638\n",
      "[389] train_loss: 0.539\n",
      "[389] val_loss: 0.638\n",
      "[390] train_loss: 0.532\n",
      "[390] val_loss: 0.638\n",
      "[391] train_loss: 0.540\n",
      "[391] val_loss: 0.638\n",
      "[392] train_loss: 0.533\n",
      "[392] val_loss: 0.638\n",
      "[393] train_loss: 0.548\n",
      "[393] val_loss: 0.638\n",
      "[394] train_loss: 0.549\n",
      "[394] val_loss: 0.638\n",
      "[395] train_loss: 0.532\n",
      "[395] val_loss: 0.638\n",
      "[396] train_loss: 0.539\n",
      "[396] val_loss: 0.638\n",
      "[397] train_loss: 0.545\n",
      "[397] val_loss: 0.638\n",
      "[398] train_loss: 0.530\n",
      "[398] val_loss: 0.638\n",
      "[399] train_loss: 0.535\n",
      "[399] val_loss: 0.638\n",
      "[400] train_loss: 0.543\n",
      "[400] val_loss: 0.638\n",
      "[401] train_loss: 0.535\n",
      "[401] val_loss: 0.638\n",
      "[402] train_loss: 0.538\n",
      "[402] val_loss: 0.638\n",
      "[403] train_loss: 0.527\n",
      "[403] val_loss: 0.638\n",
      "[404] train_loss: 0.542\n",
      "[404] val_loss: 0.638\n",
      "[405] train_loss: 0.542\n",
      "[405] val_loss: 0.638\n",
      "[406] train_loss: 0.543\n",
      "[406] val_loss: 0.638\n",
      "[407] train_loss: 0.541\n",
      "[407] val_loss: 0.638\n",
      "[408] train_loss: 0.542\n",
      "[408] val_loss: 0.638\n",
      "[409] train_loss: 0.542\n",
      "[409] val_loss: 0.638\n",
      "[410] train_loss: 0.543\n",
      "[410] val_loss: 0.638\n",
      "[411] train_loss: 0.531\n",
      "[411] val_loss: 0.638\n",
      "[412] train_loss: 0.546\n",
      "[412] val_loss: 0.638\n",
      "[413] train_loss: 0.536\n",
      "[413] val_loss: 0.638\n",
      "[414] train_loss: 0.552\n",
      "[414] val_loss: 0.638\n",
      "[415] train_loss: 0.538\n",
      "[415] val_loss: 0.638\n",
      "[416] train_loss: 0.549\n",
      "[416] val_loss: 0.638\n",
      "[417] train_loss: 0.541\n",
      "[417] val_loss: 0.638\n",
      "[418] train_loss: 0.547\n",
      "[418] val_loss: 0.638\n",
      "[419] train_loss: 0.546\n",
      "[419] val_loss: 0.638\n",
      "[420] train_loss: 0.544\n",
      "[420] val_loss: 0.638\n",
      "[421] train_loss: 0.547\n",
      "[421] val_loss: 0.638\n",
      "[422] train_loss: 0.542\n",
      "[422] val_loss: 0.638\n",
      "[423] train_loss: 0.544\n",
      "[423] val_loss: 0.638\n",
      "[424] train_loss: 0.538\n",
      "[424] val_loss: 0.638\n",
      "[425] train_loss: 0.567\n",
      "[425] val_loss: 0.638\n",
      "[426] train_loss: 0.531\n",
      "[426] val_loss: 0.638\n",
      "[427] train_loss: 0.529\n",
      "[427] val_loss: 0.638\n",
      "[428] train_loss: 0.535\n",
      "[428] val_loss: 0.638\n",
      "[429] train_loss: 0.537\n",
      "[429] val_loss: 0.638\n",
      "[430] train_loss: 0.533\n",
      "[430] val_loss: 0.638\n",
      "[431] train_loss: 0.558\n",
      "[431] val_loss: 0.638\n",
      "[432] train_loss: 0.536\n",
      "[432] val_loss: 0.638\n",
      "[433] train_loss: 0.541\n",
      "[433] val_loss: 0.638\n",
      "[434] train_loss: 0.542\n",
      "[434] val_loss: 0.638\n",
      "[435] train_loss: 0.535\n",
      "[435] val_loss: 0.638\n",
      "[436] train_loss: 0.539\n",
      "[436] val_loss: 0.638\n",
      "[437] train_loss: 0.543\n",
      "[437] val_loss: 0.638\n",
      "[438] train_loss: 0.548\n",
      "[438] val_loss: 0.638\n",
      "[439] train_loss: 0.546\n",
      "[439] val_loss: 0.638\n",
      "[440] train_loss: 0.538\n",
      "[440] val_loss: 0.638\n",
      "[441] train_loss: 0.531\n",
      "[441] val_loss: 0.638\n",
      "[442] train_loss: 0.552\n",
      "[442] val_loss: 0.638\n",
      "[443] train_loss: 0.536\n",
      "[443] val_loss: 0.638\n",
      "[444] train_loss: 0.533\n",
      "[444] val_loss: 0.638\n",
      "[445] train_loss: 0.535\n",
      "[445] val_loss: 0.638\n",
      "[446] train_loss: 0.527\n",
      "[446] val_loss: 0.638\n",
      "[447] train_loss: 0.539\n",
      "[447] val_loss: 0.638\n",
      "[448] train_loss: 0.550\n",
      "[448] val_loss: 0.638\n",
      "[449] train_loss: 0.539\n",
      "[449] val_loss: 0.638\n",
      "[450] train_loss: 0.540\n",
      "[450] val_loss: 0.638\n",
      "[451] train_loss: 0.546\n",
      "[451] val_loss: 0.638\n",
      "[452] train_loss: 0.536\n",
      "[452] val_loss: 0.638\n",
      "[453] train_loss: 0.531\n",
      "[453] val_loss: 0.638\n",
      "[454] train_loss: 0.536\n",
      "[454] val_loss: 0.638\n",
      "[455] train_loss: 0.550\n",
      "[455] val_loss: 0.638\n",
      "[456] train_loss: 0.534\n",
      "[456] val_loss: 0.638\n",
      "[457] train_loss: 0.541\n",
      "[457] val_loss: 0.638\n",
      "[458] train_loss: 0.548\n",
      "[458] val_loss: 0.638\n",
      "[459] train_loss: 0.531\n",
      "[459] val_loss: 0.638\n",
      "[460] train_loss: 0.531\n",
      "[460] val_loss: 0.638\n",
      "[461] train_loss: 0.532\n",
      "[461] val_loss: 0.638\n",
      "[462] train_loss: 0.530\n",
      "[462] val_loss: 0.638\n",
      "[463] train_loss: 0.539\n",
      "[463] val_loss: 0.638\n",
      "[464] train_loss: 0.533\n",
      "[464] val_loss: 0.638\n",
      "[465] train_loss: 0.548\n",
      "[465] val_loss: 0.638\n",
      "[466] train_loss: 0.542\n",
      "[466] val_loss: 0.638\n",
      "[467] train_loss: 0.539\n",
      "[467] val_loss: 0.638\n",
      "[468] train_loss: 0.547\n",
      "[468] val_loss: 0.638\n",
      "[469] train_loss: 0.548\n",
      "[469] val_loss: 0.638\n",
      "[470] train_loss: 0.543\n",
      "[470] val_loss: 0.638\n",
      "[471] train_loss: 0.534\n",
      "[471] val_loss: 0.638\n",
      "[472] train_loss: 0.543\n",
      "[472] val_loss: 0.638\n",
      "[473] train_loss: 0.546\n",
      "[473] val_loss: 0.638\n",
      "[474] train_loss: 0.542\n",
      "[474] val_loss: 0.638\n",
      "[475] train_loss: 0.539\n",
      "[475] val_loss: 0.638\n",
      "[476] train_loss: 0.542\n",
      "[476] val_loss: 0.638\n",
      "[477] train_loss: 0.535\n",
      "[477] val_loss: 0.638\n",
      "[478] train_loss: 0.529\n",
      "[478] val_loss: 0.638\n",
      "[479] train_loss: 0.542\n",
      "[479] val_loss: 0.638\n",
      "[480] train_loss: 0.531\n",
      "[480] val_loss: 0.638\n",
      "[481] train_loss: 0.540\n",
      "[481] val_loss: 0.638\n",
      "[482] train_loss: 0.534\n",
      "[482] val_loss: 0.638\n",
      "[483] train_loss: 0.532\n",
      "[483] val_loss: 0.638\n",
      "[484] train_loss: 0.525\n",
      "[484] val_loss: 0.638\n",
      "[485] train_loss: 0.537\n",
      "[485] val_loss: 0.638\n",
      "[486] train_loss: 0.538\n",
      "[486] val_loss: 0.638\n",
      "[487] train_loss: 0.542\n",
      "[487] val_loss: 0.638\n",
      "[488] train_loss: 0.541\n",
      "[488] val_loss: 0.638\n",
      "[489] train_loss: 0.545\n",
      "[489] val_loss: 0.638\n",
      "[490] train_loss: 0.529\n",
      "[490] val_loss: 0.638\n",
      "[491] train_loss: 0.542\n",
      "[491] val_loss: 0.638\n",
      "[492] train_loss: 0.538\n",
      "[492] val_loss: 0.638\n",
      "[493] train_loss: 0.531\n",
      "[493] val_loss: 0.638\n",
      "[494] train_loss: 0.541\n",
      "[494] val_loss: 0.638\n",
      "[495] train_loss: 0.546\n",
      "[495] val_loss: 0.638\n",
      "[496] train_loss: 0.537\n",
      "[496] val_loss: 0.638\n",
      "[497] train_loss: 0.533\n",
      "[497] val_loss: 0.638\n",
      "[498] train_loss: 0.545\n",
      "[498] val_loss: 0.638\n",
      "[499] train_loss: 0.546\n",
      "[499] val_loss: 0.638\n",
      "[500] train_loss: 0.545\n",
      "[500] val_loss: 0.638\n",
      "[501] train_loss: 0.546\n",
      "[501] val_loss: 0.638\n",
      "[502] train_loss: 0.543\n",
      "[502] val_loss: 0.638\n",
      "[503] train_loss: 0.549\n",
      "[503] val_loss: 0.638\n",
      "[504] train_loss: 0.546\n",
      "[504] val_loss: 0.638\n",
      "[505] train_loss: 0.535\n",
      "[505] val_loss: 0.638\n",
      "[506] train_loss: 0.552\n",
      "[506] val_loss: 0.638\n",
      "[507] train_loss: 0.546\n",
      "[507] val_loss: 0.638\n",
      "[508] train_loss: 0.538\n",
      "[508] val_loss: 0.638\n",
      "[509] train_loss: 0.535\n",
      "[509] val_loss: 0.638\n",
      "[510] train_loss: 0.556\n",
      "[510] val_loss: 0.638\n",
      "[511] train_loss: 0.567\n",
      "[511] val_loss: 0.638\n",
      "[512] train_loss: 0.533\n",
      "[512] val_loss: 0.638\n",
      "[513] train_loss: 0.539\n",
      "[513] val_loss: 0.638\n",
      "[514] train_loss: 0.546\n",
      "[514] val_loss: 0.638\n",
      "[515] train_loss: 0.550\n",
      "[515] val_loss: 0.638\n",
      "[516] train_loss: 0.537\n",
      "[516] val_loss: 0.638\n",
      "[517] train_loss: 0.541\n",
      "[517] val_loss: 0.638\n",
      "[518] train_loss: 0.553\n",
      "[518] val_loss: 0.638\n",
      "[519] train_loss: 0.528\n",
      "[519] val_loss: 0.638\n",
      "[520] train_loss: 0.535\n",
      "[520] val_loss: 0.638\n",
      "[521] train_loss: 0.544\n",
      "[521] val_loss: 0.638\n",
      "[522] train_loss: 0.541\n",
      "[522] val_loss: 0.638\n",
      "[523] train_loss: 0.542\n",
      "[523] val_loss: 0.638\n",
      "[524] train_loss: 0.548\n",
      "[524] val_loss: 0.638\n",
      "[525] train_loss: 0.540\n",
      "[525] val_loss: 0.638\n",
      "[526] train_loss: 0.546\n",
      "[526] val_loss: 0.638\n",
      "[527] train_loss: 0.539\n",
      "[527] val_loss: 0.638\n",
      "[528] train_loss: 0.534\n",
      "[528] val_loss: 0.638\n",
      "[529] train_loss: 0.543\n",
      "[529] val_loss: 0.638\n",
      "[530] train_loss: 0.534\n",
      "[530] val_loss: 0.638\n",
      "[531] train_loss: 0.542\n",
      "[531] val_loss: 0.638\n",
      "[532] train_loss: 0.540\n",
      "[532] val_loss: 0.638\n",
      "[533] train_loss: 0.539\n",
      "[533] val_loss: 0.638\n",
      "[534] train_loss: 0.541\n",
      "[534] val_loss: 0.638\n",
      "[535] train_loss: 0.548\n",
      "[535] val_loss: 0.638\n",
      "[536] train_loss: 0.541\n",
      "[536] val_loss: 0.638\n",
      "[537] train_loss: 0.546\n",
      "[537] val_loss: 0.638\n",
      "[538] train_loss: 0.548\n",
      "[538] val_loss: 0.638\n",
      "[539] train_loss: 0.541\n",
      "[539] val_loss: 0.638\n",
      "[540] train_loss: 0.542\n",
      "[540] val_loss: 0.638\n",
      "[541] train_loss: 0.542\n",
      "[541] val_loss: 0.638\n",
      "[542] train_loss: 0.547\n",
      "[542] val_loss: 0.638\n",
      "[543] train_loss: 0.542\n",
      "[543] val_loss: 0.638\n",
      "[544] train_loss: 0.557\n",
      "[544] val_loss: 0.638\n",
      "[545] train_loss: 0.560\n",
      "[545] val_loss: 0.638\n",
      "[546] train_loss: 0.535\n",
      "[546] val_loss: 0.638\n",
      "[547] train_loss: 0.527\n",
      "[547] val_loss: 0.638\n",
      "[548] train_loss: 0.538\n",
      "[548] val_loss: 0.638\n",
      "[549] train_loss: 0.534\n",
      "[549] val_loss: 0.638\n",
      "[550] train_loss: 0.542\n",
      "[550] val_loss: 0.638\n",
      "[551] train_loss: 0.537\n",
      "[551] val_loss: 0.638\n",
      "[552] train_loss: 0.542\n",
      "[552] val_loss: 0.638\n",
      "[553] train_loss: 0.550\n",
      "[553] val_loss: 0.638\n",
      "[554] train_loss: 0.546\n",
      "[554] val_loss: 0.638\n",
      "[555] train_loss: 0.545\n",
      "[555] val_loss: 0.638\n",
      "[556] train_loss: 0.527\n",
      "[556] val_loss: 0.638\n",
      "[557] train_loss: 0.557\n",
      "[557] val_loss: 0.638\n",
      "[558] train_loss: 0.535\n",
      "[558] val_loss: 0.638\n",
      "[559] train_loss: 0.543\n",
      "[559] val_loss: 0.638\n",
      "[560] train_loss: 0.535\n",
      "[560] val_loss: 0.638\n",
      "[561] train_loss: 0.546\n",
      "[561] val_loss: 0.638\n",
      "[562] train_loss: 0.541\n",
      "[562] val_loss: 0.638\n",
      "[563] train_loss: 0.534\n",
      "[563] val_loss: 0.638\n",
      "[564] train_loss: 0.560\n",
      "[564] val_loss: 0.638\n",
      "[565] train_loss: 0.548\n",
      "[565] val_loss: 0.638\n",
      "[566] train_loss: 0.535\n",
      "[566] val_loss: 0.638\n",
      "[567] train_loss: 0.545\n",
      "[567] val_loss: 0.638\n",
      "[568] train_loss: 0.554\n",
      "[568] val_loss: 0.638\n",
      "[569] train_loss: 0.532\n",
      "[569] val_loss: 0.638\n",
      "[570] train_loss: 0.535\n",
      "[570] val_loss: 0.638\n",
      "[571] train_loss: 0.553\n",
      "[571] val_loss: 0.638\n",
      "[572] train_loss: 0.538\n",
      "[572] val_loss: 0.638\n",
      "[573] train_loss: 0.531\n",
      "[573] val_loss: 0.638\n",
      "[574] train_loss: 0.538\n",
      "[574] val_loss: 0.638\n",
      "[575] train_loss: 0.538\n",
      "[575] val_loss: 0.638\n",
      "[576] train_loss: 0.533\n",
      "[576] val_loss: 0.638\n",
      "[577] train_loss: 0.532\n",
      "[577] val_loss: 0.638\n",
      "[578] train_loss: 0.546\n",
      "[578] val_loss: 0.638\n",
      "[579] train_loss: 0.541\n",
      "[579] val_loss: 0.638\n",
      "[580] train_loss: 0.527\n",
      "[580] val_loss: 0.638\n",
      "[581] train_loss: 0.527\n",
      "[581] val_loss: 0.638\n",
      "[582] train_loss: 0.530\n",
      "[582] val_loss: 0.638\n",
      "[583] train_loss: 0.527\n",
      "[583] val_loss: 0.638\n",
      "[584] train_loss: 0.545\n",
      "[584] val_loss: 0.638\n",
      "[585] train_loss: 0.530\n",
      "[585] val_loss: 0.638\n",
      "[586] train_loss: 0.526\n",
      "[586] val_loss: 0.638\n",
      "[587] train_loss: 0.541\n",
      "[587] val_loss: 0.638\n",
      "[588] train_loss: 0.546\n",
      "[588] val_loss: 0.638\n",
      "[589] train_loss: 0.536\n",
      "[589] val_loss: 0.638\n",
      "[590] train_loss: 0.553\n",
      "[590] val_loss: 0.638\n",
      "[591] train_loss: 0.523\n",
      "[591] val_loss: 0.638\n",
      "[592] train_loss: 0.528\n",
      "[592] val_loss: 0.638\n",
      "[593] train_loss: 0.533\n",
      "[593] val_loss: 0.638\n",
      "[594] train_loss: 0.534\n",
      "[594] val_loss: 0.638\n",
      "[595] train_loss: 0.543\n",
      "[595] val_loss: 0.638\n",
      "[596] train_loss: 0.538\n",
      "[596] val_loss: 0.638\n",
      "[597] train_loss: 0.534\n",
      "[597] val_loss: 0.638\n",
      "[598] train_loss: 0.548\n",
      "[598] val_loss: 0.638\n",
      "[599] train_loss: 0.530\n",
      "[599] val_loss: 0.638\n",
      "[600] train_loss: 0.537\n",
      "[600] val_loss: 0.638\n",
      "[601] train_loss: 0.531\n",
      "[601] val_loss: 0.638\n",
      "[602] train_loss: 0.537\n",
      "[602] val_loss: 0.638\n",
      "[603] train_loss: 0.551\n",
      "[603] val_loss: 0.638\n",
      "[604] train_loss: 0.541\n",
      "[604] val_loss: 0.638\n",
      "[605] train_loss: 0.544\n",
      "[605] val_loss: 0.638\n",
      "[606] train_loss: 0.547\n",
      "[606] val_loss: 0.638\n",
      "[607] train_loss: 0.554\n",
      "[607] val_loss: 0.638\n",
      "[608] train_loss: 0.545\n",
      "[608] val_loss: 0.638\n",
      "[609] train_loss: 0.545\n",
      "[609] val_loss: 0.638\n",
      "[610] train_loss: 0.545\n",
      "[610] val_loss: 0.638\n",
      "[611] train_loss: 0.542\n",
      "[611] val_loss: 0.638\n",
      "[612] train_loss: 0.534\n",
      "[612] val_loss: 0.638\n",
      "[613] train_loss: 0.553\n",
      "[613] val_loss: 0.638\n",
      "[614] train_loss: 0.531\n",
      "[614] val_loss: 0.638\n",
      "[615] train_loss: 0.531\n",
      "[615] val_loss: 0.638\n",
      "[616] train_loss: 0.529\n",
      "[616] val_loss: 0.638\n",
      "[617] train_loss: 0.551\n",
      "[617] val_loss: 0.638\n",
      "[618] train_loss: 0.544\n",
      "[618] val_loss: 0.638\n",
      "[619] train_loss: 0.534\n",
      "[619] val_loss: 0.638\n",
      "[620] train_loss: 0.556\n",
      "[620] val_loss: 0.638\n",
      "[621] train_loss: 0.536\n",
      "[621] val_loss: 0.638\n",
      "[622] train_loss: 0.554\n",
      "[622] val_loss: 0.638\n",
      "[623] train_loss: 0.537\n",
      "[623] val_loss: 0.638\n",
      "[624] train_loss: 0.542\n",
      "[624] val_loss: 0.638\n",
      "[625] train_loss: 0.536\n",
      "[625] val_loss: 0.638\n",
      "[626] train_loss: 0.541\n",
      "[626] val_loss: 0.638\n",
      "[627] train_loss: 0.546\n",
      "[627] val_loss: 0.638\n",
      "[628] train_loss: 0.530\n",
      "[628] val_loss: 0.638\n",
      "[629] train_loss: 0.533\n",
      "[629] val_loss: 0.638\n",
      "[630] train_loss: 0.545\n",
      "[630] val_loss: 0.638\n",
      "[631] train_loss: 0.540\n",
      "[631] val_loss: 0.638\n",
      "[632] train_loss: 0.538\n",
      "[632] val_loss: 0.638\n",
      "[633] train_loss: 0.534\n",
      "[633] val_loss: 0.638\n",
      "[634] train_loss: 0.532\n",
      "[634] val_loss: 0.638\n",
      "[635] train_loss: 0.545\n",
      "[635] val_loss: 0.638\n",
      "[636] train_loss: 0.550\n",
      "[636] val_loss: 0.638\n",
      "[637] train_loss: 0.534\n",
      "[637] val_loss: 0.638\n",
      "[638] train_loss: 0.545\n",
      "[638] val_loss: 0.638\n",
      "[639] train_loss: 0.551\n",
      "[639] val_loss: 0.638\n",
      "[640] train_loss: 0.540\n",
      "[640] val_loss: 0.638\n",
      "[641] train_loss: 0.532\n",
      "[641] val_loss: 0.638\n",
      "[642] train_loss: 0.534\n",
      "[642] val_loss: 0.638\n",
      "[643] train_loss: 0.535\n",
      "[643] val_loss: 0.638\n",
      "[644] train_loss: 0.539\n",
      "[644] val_loss: 0.638\n",
      "[645] train_loss: 0.538\n",
      "[645] val_loss: 0.638\n",
      "[646] train_loss: 0.535\n",
      "[646] val_loss: 0.638\n",
      "[647] train_loss: 0.538\n",
      "[647] val_loss: 0.638\n",
      "[648] train_loss: 0.552\n",
      "[648] val_loss: 0.638\n",
      "[649] train_loss: 0.534\n",
      "[649] val_loss: 0.638\n",
      "[650] train_loss: 0.533\n",
      "[650] val_loss: 0.638\n",
      "[651] train_loss: 0.530\n",
      "[651] val_loss: 0.638\n",
      "[652] train_loss: 0.547\n",
      "[652] val_loss: 0.638\n",
      "[653] train_loss: 0.548\n",
      "[653] val_loss: 0.638\n",
      "[654] train_loss: 0.535\n",
      "[654] val_loss: 0.638\n",
      "[655] train_loss: 0.537\n",
      "[655] val_loss: 0.638\n",
      "[656] train_loss: 0.538\n",
      "[656] val_loss: 0.638\n",
      "[657] train_loss: 0.541\n",
      "[657] val_loss: 0.638\n",
      "[658] train_loss: 0.542\n",
      "[658] val_loss: 0.638\n",
      "[659] train_loss: 0.528\n",
      "[659] val_loss: 0.638\n",
      "[660] train_loss: 0.548\n",
      "[660] val_loss: 0.638\n",
      "[661] train_loss: 0.543\n",
      "[661] val_loss: 0.638\n",
      "[662] train_loss: 0.551\n",
      "[662] val_loss: 0.638\n",
      "[663] train_loss: 0.542\n",
      "[663] val_loss: 0.638\n",
      "[664] train_loss: 0.533\n",
      "[664] val_loss: 0.638\n",
      "[665] train_loss: 0.538\n",
      "[665] val_loss: 0.638\n",
      "[666] train_loss: 0.526\n",
      "[666] val_loss: 0.638\n",
      "[667] train_loss: 0.544\n",
      "[667] val_loss: 0.638\n",
      "[668] train_loss: 0.546\n",
      "[668] val_loss: 0.638\n",
      "[669] train_loss: 0.542\n",
      "[669] val_loss: 0.638\n",
      "[670] train_loss: 0.534\n",
      "[670] val_loss: 0.638\n",
      "[671] train_loss: 0.547\n",
      "[671] val_loss: 0.638\n",
      "[672] train_loss: 0.546\n",
      "[672] val_loss: 0.638\n",
      "[673] train_loss: 0.539\n",
      "[673] val_loss: 0.638\n",
      "[674] train_loss: 0.549\n",
      "[674] val_loss: 0.638\n",
      "[675] train_loss: 0.554\n",
      "[675] val_loss: 0.638\n",
      "[676] train_loss: 0.544\n",
      "[676] val_loss: 0.638\n",
      "[677] train_loss: 0.543\n",
      "[677] val_loss: 0.638\n",
      "[678] train_loss: 0.535\n",
      "[678] val_loss: 0.638\n",
      "[679] train_loss: 0.543\n",
      "[679] val_loss: 0.638\n",
      "[680] train_loss: 0.534\n",
      "[680] val_loss: 0.638\n",
      "[681] train_loss: 0.526\n",
      "[681] val_loss: 0.638\n",
      "[682] train_loss: 0.543\n",
      "[682] val_loss: 0.638\n",
      "[683] train_loss: 0.537\n",
      "[683] val_loss: 0.638\n",
      "[684] train_loss: 0.552\n",
      "[684] val_loss: 0.638\n",
      "[685] train_loss: 0.544\n",
      "[685] val_loss: 0.638\n",
      "[686] train_loss: 0.540\n",
      "[686] val_loss: 0.638\n",
      "[687] train_loss: 0.545\n",
      "[687] val_loss: 0.638\n",
      "[688] train_loss: 0.533\n",
      "[688] val_loss: 0.638\n",
      "[689] train_loss: 0.556\n",
      "[689] val_loss: 0.638\n",
      "[690] train_loss: 0.532\n",
      "[690] val_loss: 0.638\n",
      "[691] train_loss: 0.541\n",
      "[691] val_loss: 0.638\n",
      "[692] train_loss: 0.541\n",
      "[692] val_loss: 0.638\n",
      "[693] train_loss: 0.539\n",
      "[693] val_loss: 0.638\n",
      "[694] train_loss: 0.538\n",
      "[694] val_loss: 0.638\n",
      "[695] train_loss: 0.539\n",
      "[695] val_loss: 0.638\n",
      "[696] train_loss: 0.557\n",
      "[696] val_loss: 0.638\n",
      "[697] train_loss: 0.534\n",
      "[697] val_loss: 0.638\n",
      "[698] train_loss: 0.534\n",
      "[698] val_loss: 0.638\n",
      "[699] train_loss: 0.545\n",
      "[699] val_loss: 0.638\n",
      "[700] train_loss: 0.527\n",
      "[700] val_loss: 0.638\n",
      "[701] train_loss: 0.538\n",
      "[701] val_loss: 0.638\n",
      "[702] train_loss: 0.545\n",
      "[702] val_loss: 0.638\n",
      "[703] train_loss: 0.535\n",
      "[703] val_loss: 0.638\n",
      "[704] train_loss: 0.550\n",
      "[704] val_loss: 0.638\n",
      "[705] train_loss: 0.549\n",
      "[705] val_loss: 0.638\n",
      "[706] train_loss: 0.538\n",
      "[706] val_loss: 0.638\n",
      "[707] train_loss: 0.528\n",
      "[707] val_loss: 0.638\n",
      "[708] train_loss: 0.549\n",
      "[708] val_loss: 0.638\n",
      "[709] train_loss: 0.553\n",
      "[709] val_loss: 0.638\n",
      "[710] train_loss: 0.536\n",
      "[710] val_loss: 0.638\n",
      "[711] train_loss: 0.542\n",
      "[711] val_loss: 0.638\n",
      "[712] train_loss: 0.539\n",
      "[712] val_loss: 0.638\n",
      "[713] train_loss: 0.546\n",
      "[713] val_loss: 0.638\n",
      "[714] train_loss: 0.535\n",
      "[714] val_loss: 0.638\n",
      "[715] train_loss: 0.551\n",
      "[715] val_loss: 0.638\n",
      "[716] train_loss: 0.533\n",
      "[716] val_loss: 0.638\n",
      "[717] train_loss: 0.539\n",
      "[717] val_loss: 0.638\n",
      "[718] train_loss: 0.547\n",
      "[718] val_loss: 0.638\n",
      "[719] train_loss: 0.553\n",
      "[719] val_loss: 0.638\n",
      "[720] train_loss: 0.531\n",
      "[720] val_loss: 0.638\n",
      "[721] train_loss: 0.542\n",
      "[721] val_loss: 0.638\n",
      "[722] train_loss: 0.545\n",
      "[722] val_loss: 0.638\n",
      "[723] train_loss: 0.536\n",
      "[723] val_loss: 0.638\n",
      "[724] train_loss: 0.550\n",
      "[724] val_loss: 0.638\n",
      "[725] train_loss: 0.549\n",
      "[725] val_loss: 0.638\n",
      "[726] train_loss: 0.539\n",
      "[726] val_loss: 0.638\n",
      "[727] train_loss: 0.547\n",
      "[727] val_loss: 0.638\n",
      "[728] train_loss: 0.541\n",
      "[728] val_loss: 0.638\n",
      "[729] train_loss: 0.558\n",
      "[729] val_loss: 0.638\n",
      "[730] train_loss: 0.528\n",
      "[730] val_loss: 0.638\n",
      "[731] train_loss: 0.556\n",
      "[731] val_loss: 0.638\n",
      "[732] train_loss: 0.523\n",
      "[732] val_loss: 0.638\n",
      "[733] train_loss: 0.541\n",
      "[733] val_loss: 0.638\n",
      "[734] train_loss: 0.553\n",
      "[734] val_loss: 0.638\n",
      "[735] train_loss: 0.544\n",
      "[735] val_loss: 0.638\n",
      "[736] train_loss: 0.549\n",
      "[736] val_loss: 0.638\n",
      "[737] train_loss: 0.525\n",
      "[737] val_loss: 0.638\n",
      "[738] train_loss: 0.552\n",
      "[738] val_loss: 0.638\n",
      "[739] train_loss: 0.549\n",
      "[739] val_loss: 0.638\n",
      "[740] train_loss: 0.546\n",
      "[740] val_loss: 0.638\n",
      "[741] train_loss: 0.543\n",
      "[741] val_loss: 0.638\n",
      "[742] train_loss: 0.530\n",
      "[742] val_loss: 0.638\n",
      "[743] train_loss: 0.532\n",
      "[743] val_loss: 0.638\n",
      "[744] train_loss: 0.553\n",
      "[744] val_loss: 0.638\n",
      "[745] train_loss: 0.547\n",
      "[745] val_loss: 0.638\n",
      "[746] train_loss: 0.549\n",
      "[746] val_loss: 0.638\n",
      "[747] train_loss: 0.540\n",
      "[747] val_loss: 0.638\n",
      "[748] train_loss: 0.545\n",
      "[748] val_loss: 0.638\n",
      "[749] train_loss: 0.540\n",
      "[749] val_loss: 0.638\n",
      "[750] train_loss: 0.553\n",
      "[750] val_loss: 0.638\n",
      "[751] train_loss: 0.553\n",
      "[751] val_loss: 0.638\n",
      "[752] train_loss: 0.553\n",
      "[752] val_loss: 0.638\n",
      "[753] train_loss: 0.541\n",
      "[753] val_loss: 0.638\n",
      "[754] train_loss: 0.533\n",
      "[754] val_loss: 0.638\n",
      "[755] train_loss: 0.544\n",
      "[755] val_loss: 0.638\n",
      "[756] train_loss: 0.542\n",
      "[756] val_loss: 0.638\n",
      "[757] train_loss: 0.545\n",
      "[757] val_loss: 0.638\n",
      "[758] train_loss: 0.537\n",
      "[758] val_loss: 0.638\n",
      "[759] train_loss: 0.545\n",
      "[759] val_loss: 0.638\n",
      "[760] train_loss: 0.538\n",
      "[760] val_loss: 0.638\n",
      "[761] train_loss: 0.536\n",
      "[761] val_loss: 0.638\n",
      "[762] train_loss: 0.539\n",
      "[762] val_loss: 0.638\n",
      "[763] train_loss: 0.545\n",
      "[763] val_loss: 0.638\n",
      "[764] train_loss: 0.536\n",
      "[764] val_loss: 0.638\n",
      "[765] train_loss: 0.538\n",
      "[765] val_loss: 0.638\n",
      "[766] train_loss: 0.552\n",
      "[766] val_loss: 0.638\n",
      "[767] train_loss: 0.541\n",
      "[767] val_loss: 0.638\n",
      "[768] train_loss: 0.545\n",
      "[768] val_loss: 0.638\n",
      "[769] train_loss: 0.529\n",
      "[769] val_loss: 0.638\n",
      "[770] train_loss: 0.546\n",
      "[770] val_loss: 0.638\n",
      "[771] train_loss: 0.556\n",
      "[771] val_loss: 0.638\n",
      "[772] train_loss: 0.533\n",
      "[772] val_loss: 0.638\n",
      "[773] train_loss: 0.550\n",
      "[773] val_loss: 0.638\n",
      "[774] train_loss: 0.536\n",
      "[774] val_loss: 0.638\n",
      "[775] train_loss: 0.542\n",
      "[775] val_loss: 0.638\n",
      "[776] train_loss: 0.530\n",
      "[776] val_loss: 0.638\n",
      "[777] train_loss: 0.549\n",
      "[777] val_loss: 0.638\n",
      "[778] train_loss: 0.555\n",
      "[778] val_loss: 0.638\n",
      "[779] train_loss: 0.535\n",
      "[779] val_loss: 0.638\n",
      "[780] train_loss: 0.548\n",
      "[780] val_loss: 0.638\n",
      "[781] train_loss: 0.538\n",
      "[781] val_loss: 0.638\n",
      "[782] train_loss: 0.535\n",
      "[782] val_loss: 0.638\n",
      "[783] train_loss: 0.531\n",
      "[783] val_loss: 0.638\n",
      "[784] train_loss: 0.549\n",
      "[784] val_loss: 0.638\n",
      "[785] train_loss: 0.546\n",
      "[785] val_loss: 0.638\n",
      "[786] train_loss: 0.539\n",
      "[786] val_loss: 0.638\n",
      "[787] train_loss: 0.543\n",
      "[787] val_loss: 0.638\n",
      "[788] train_loss: 0.546\n",
      "[788] val_loss: 0.638\n",
      "[789] train_loss: 0.542\n",
      "[789] val_loss: 0.638\n",
      "[790] train_loss: 0.531\n",
      "[790] val_loss: 0.638\n",
      "[791] train_loss: 0.525\n",
      "[791] val_loss: 0.638\n",
      "[792] train_loss: 0.538\n",
      "[792] val_loss: 0.638\n",
      "[793] train_loss: 0.542\n",
      "[793] val_loss: 0.638\n",
      "[794] train_loss: 0.539\n",
      "[794] val_loss: 0.638\n",
      "[795] train_loss: 0.542\n",
      "[795] val_loss: 0.638\n",
      "[796] train_loss: 0.542\n",
      "[796] val_loss: 0.638\n",
      "[797] train_loss: 0.546\n",
      "[797] val_loss: 0.638\n",
      "[798] train_loss: 0.545\n",
      "[798] val_loss: 0.638\n",
      "[799] train_loss: 0.549\n",
      "[799] val_loss: 0.638\n",
      "[800] train_loss: 0.531\n",
      "[800] val_loss: 0.638\n",
      "[801] train_loss: 0.542\n",
      "[801] val_loss: 0.638\n",
      "[802] train_loss: 0.533\n",
      "[802] val_loss: 0.638\n",
      "[803] train_loss: 0.538\n",
      "[803] val_loss: 0.638\n",
      "[804] train_loss: 0.535\n",
      "[804] val_loss: 0.638\n",
      "[805] train_loss: 0.548\n",
      "[805] val_loss: 0.638\n",
      "[806] train_loss: 0.527\n",
      "[806] val_loss: 0.638\n",
      "[807] train_loss: 0.531\n",
      "[807] val_loss: 0.638\n",
      "[808] train_loss: 0.539\n",
      "[808] val_loss: 0.638\n",
      "[809] train_loss: 0.531\n",
      "[809] val_loss: 0.638\n",
      "[810] train_loss: 0.542\n",
      "[810] val_loss: 0.638\n",
      "[811] train_loss: 0.537\n",
      "[811] val_loss: 0.638\n",
      "[812] train_loss: 0.535\n",
      "[812] val_loss: 0.638\n",
      "[813] train_loss: 0.524\n",
      "[813] val_loss: 0.638\n",
      "[814] train_loss: 0.549\n",
      "[814] val_loss: 0.638\n",
      "[815] train_loss: 0.529\n",
      "[815] val_loss: 0.638\n",
      "[816] train_loss: 0.538\n",
      "[816] val_loss: 0.638\n",
      "[817] train_loss: 0.539\n",
      "[817] val_loss: 0.638\n",
      "[818] train_loss: 0.542\n",
      "[818] val_loss: 0.638\n",
      "[819] train_loss: 0.540\n",
      "[819] val_loss: 0.638\n",
      "[820] train_loss: 0.546\n",
      "[820] val_loss: 0.638\n",
      "[821] train_loss: 0.535\n",
      "[821] val_loss: 0.638\n",
      "[822] train_loss: 0.546\n",
      "[822] val_loss: 0.638\n",
      "[823] train_loss: 0.539\n",
      "[823] val_loss: 0.638\n",
      "[824] train_loss: 0.545\n",
      "[824] val_loss: 0.638\n",
      "[825] train_loss: 0.546\n",
      "[825] val_loss: 0.638\n",
      "[826] train_loss: 0.538\n",
      "[826] val_loss: 0.638\n",
      "[827] train_loss: 0.539\n",
      "[827] val_loss: 0.638\n",
      "[828] train_loss: 0.541\n",
      "[828] val_loss: 0.638\n",
      "[829] train_loss: 0.544\n",
      "[829] val_loss: 0.638\n",
      "[830] train_loss: 0.534\n",
      "[830] val_loss: 0.638\n",
      "[831] train_loss: 0.544\n",
      "[831] val_loss: 0.638\n",
      "[832] train_loss: 0.538\n",
      "[832] val_loss: 0.638\n",
      "[833] train_loss: 0.546\n",
      "[833] val_loss: 0.638\n",
      "[834] train_loss: 0.540\n",
      "[834] val_loss: 0.638\n",
      "[835] train_loss: 0.537\n",
      "[835] val_loss: 0.638\n",
      "[836] train_loss: 0.544\n",
      "[836] val_loss: 0.638\n",
      "[837] train_loss: 0.532\n",
      "[837] val_loss: 0.638\n",
      "[838] train_loss: 0.549\n",
      "[838] val_loss: 0.638\n",
      "[839] train_loss: 0.542\n",
      "[839] val_loss: 0.638\n",
      "[840] train_loss: 0.539\n",
      "[840] val_loss: 0.638\n",
      "[841] train_loss: 0.556\n",
      "[841] val_loss: 0.638\n",
      "[842] train_loss: 0.549\n",
      "[842] val_loss: 0.638\n",
      "[843] train_loss: 0.540\n",
      "[843] val_loss: 0.638\n",
      "[844] train_loss: 0.542\n",
      "[844] val_loss: 0.638\n",
      "[845] train_loss: 0.539\n",
      "[845] val_loss: 0.638\n",
      "[846] train_loss: 0.532\n",
      "[846] val_loss: 0.638\n",
      "[847] train_loss: 0.538\n",
      "[847] val_loss: 0.638\n",
      "[848] train_loss: 0.539\n",
      "[848] val_loss: 0.638\n",
      "[849] train_loss: 0.519\n",
      "[849] val_loss: 0.638\n",
      "[850] train_loss: 0.541\n",
      "[850] val_loss: 0.638\n",
      "[851] train_loss: 0.530\n",
      "[851] val_loss: 0.638\n",
      "[852] train_loss: 0.542\n",
      "[852] val_loss: 0.638\n",
      "[853] train_loss: 0.542\n",
      "[853] val_loss: 0.638\n",
      "[854] train_loss: 0.543\n",
      "[854] val_loss: 0.638\n",
      "[855] train_loss: 0.538\n",
      "[855] val_loss: 0.638\n",
      "[856] train_loss: 0.536\n",
      "[856] val_loss: 0.638\n",
      "[857] train_loss: 0.543\n",
      "[857] val_loss: 0.638\n",
      "[858] train_loss: 0.545\n",
      "[858] val_loss: 0.638\n",
      "[859] train_loss: 0.535\n",
      "[859] val_loss: 0.638\n",
      "[860] train_loss: 0.545\n",
      "[860] val_loss: 0.638\n",
      "[861] train_loss: 0.539\n",
      "[861] val_loss: 0.638\n",
      "[862] train_loss: 0.545\n",
      "[862] val_loss: 0.638\n",
      "[863] train_loss: 0.541\n",
      "[863] val_loss: 0.638\n",
      "[864] train_loss: 0.526\n",
      "[864] val_loss: 0.638\n",
      "[865] train_loss: 0.545\n",
      "[865] val_loss: 0.638\n",
      "[866] train_loss: 0.531\n",
      "[866] val_loss: 0.638\n",
      "[867] train_loss: 0.560\n",
      "[867] val_loss: 0.638\n",
      "[868] train_loss: 0.535\n",
      "[868] val_loss: 0.638\n",
      "[869] train_loss: 0.538\n",
      "[869] val_loss: 0.638\n",
      "[870] train_loss: 0.546\n",
      "[870] val_loss: 0.638\n",
      "[871] train_loss: 0.545\n",
      "[871] val_loss: 0.638\n",
      "[872] train_loss: 0.539\n",
      "[872] val_loss: 0.638\n",
      "[873] train_loss: 0.538\n",
      "[873] val_loss: 0.638\n",
      "[874] train_loss: 0.529\n",
      "[874] val_loss: 0.638\n",
      "[875] train_loss: 0.551\n",
      "[875] val_loss: 0.638\n",
      "[876] train_loss: 0.537\n",
      "[876] val_loss: 0.638\n",
      "[877] train_loss: 0.546\n",
      "[877] val_loss: 0.638\n",
      "[878] train_loss: 0.556\n",
      "[878] val_loss: 0.638\n",
      "[879] train_loss: 0.533\n",
      "[879] val_loss: 0.638\n",
      "[880] train_loss: 0.543\n",
      "[880] val_loss: 0.638\n",
      "[881] train_loss: 0.536\n",
      "[881] val_loss: 0.638\n",
      "[882] train_loss: 0.543\n",
      "[882] val_loss: 0.638\n",
      "[883] train_loss: 0.545\n",
      "[883] val_loss: 0.638\n",
      "[884] train_loss: 0.528\n",
      "[884] val_loss: 0.638\n",
      "[885] train_loss: 0.530\n",
      "[885] val_loss: 0.638\n",
      "[886] train_loss: 0.532\n",
      "[886] val_loss: 0.638\n",
      "[887] train_loss: 0.527\n",
      "[887] val_loss: 0.638\n",
      "[888] train_loss: 0.534\n",
      "[888] val_loss: 0.638\n",
      "[889] train_loss: 0.535\n",
      "[889] val_loss: 0.638\n",
      "[890] train_loss: 0.549\n",
      "[890] val_loss: 0.638\n",
      "[891] train_loss: 0.550\n",
      "[891] val_loss: 0.638\n",
      "[892] train_loss: 0.545\n",
      "[892] val_loss: 0.638\n",
      "[893] train_loss: 0.538\n",
      "[893] val_loss: 0.638\n",
      "[894] train_loss: 0.535\n",
      "[894] val_loss: 0.638\n",
      "[895] train_loss: 0.555\n",
      "[895] val_loss: 0.638\n",
      "[896] train_loss: 0.538\n",
      "[896] val_loss: 0.638\n",
      "[897] train_loss: 0.545\n",
      "[897] val_loss: 0.638\n",
      "[898] train_loss: 0.536\n",
      "[898] val_loss: 0.638\n",
      "[899] train_loss: 0.544\n",
      "[899] val_loss: 0.638\n",
      "[900] train_loss: 0.534\n",
      "[900] val_loss: 0.638\n",
      "[901] train_loss: 0.545\n",
      "[901] val_loss: 0.638\n",
      "[902] train_loss: 0.536\n",
      "[902] val_loss: 0.638\n",
      "[903] train_loss: 0.557\n",
      "[903] val_loss: 0.638\n",
      "[904] train_loss: 0.533\n",
      "[904] val_loss: 0.638\n",
      "[905] train_loss: 0.545\n",
      "[905] val_loss: 0.638\n",
      "[906] train_loss: 0.545\n",
      "[906] val_loss: 0.638\n",
      "[907] train_loss: 0.550\n",
      "[907] val_loss: 0.638\n",
      "[908] train_loss: 0.535\n",
      "[908] val_loss: 0.638\n",
      "[909] train_loss: 0.549\n",
      "[909] val_loss: 0.638\n",
      "[910] train_loss: 0.528\n",
      "[910] val_loss: 0.638\n",
      "[911] train_loss: 0.536\n",
      "[911] val_loss: 0.638\n",
      "[912] train_loss: 0.542\n",
      "[912] val_loss: 0.638\n",
      "[913] train_loss: 0.559\n",
      "[913] val_loss: 0.638\n",
      "[914] train_loss: 0.535\n",
      "[914] val_loss: 0.638\n",
      "[915] train_loss: 0.550\n",
      "[915] val_loss: 0.638\n",
      "[916] train_loss: 0.545\n",
      "[916] val_loss: 0.638\n",
      "[917] train_loss: 0.533\n",
      "[917] val_loss: 0.638\n",
      "[918] train_loss: 0.559\n",
      "[918] val_loss: 0.638\n",
      "[919] train_loss: 0.534\n",
      "[919] val_loss: 0.638\n",
      "[920] train_loss: 0.544\n",
      "[920] val_loss: 0.638\n",
      "[921] train_loss: 0.539\n",
      "[921] val_loss: 0.638\n",
      "[922] train_loss: 0.529\n",
      "[922] val_loss: 0.638\n",
      "[923] train_loss: 0.529\n",
      "[923] val_loss: 0.638\n",
      "[924] train_loss: 0.534\n",
      "[924] val_loss: 0.638\n",
      "[925] train_loss: 0.538\n",
      "[925] val_loss: 0.638\n",
      "[926] train_loss: 0.553\n",
      "[926] val_loss: 0.638\n",
      "[927] train_loss: 0.550\n",
      "[927] val_loss: 0.638\n",
      "[928] train_loss: 0.543\n",
      "[928] val_loss: 0.638\n",
      "[929] train_loss: 0.545\n",
      "[929] val_loss: 0.638\n",
      "[930] train_loss: 0.549\n",
      "[930] val_loss: 0.638\n",
      "[931] train_loss: 0.538\n",
      "[931] val_loss: 0.638\n",
      "[932] train_loss: 0.535\n",
      "[932] val_loss: 0.638\n",
      "[933] train_loss: 0.565\n",
      "[933] val_loss: 0.638\n",
      "[934] train_loss: 0.544\n",
      "[934] val_loss: 0.638\n",
      "[935] train_loss: 0.537\n",
      "[935] val_loss: 0.638\n",
      "[936] train_loss: 0.544\n",
      "[936] val_loss: 0.638\n",
      "[937] train_loss: 0.555\n",
      "[937] val_loss: 0.638\n",
      "[938] train_loss: 0.535\n",
      "[938] val_loss: 0.638\n",
      "[939] train_loss: 0.539\n",
      "[939] val_loss: 0.638\n",
      "[940] train_loss: 0.542\n",
      "[940] val_loss: 0.638\n",
      "[941] train_loss: 0.533\n",
      "[941] val_loss: 0.638\n",
      "[942] train_loss: 0.532\n",
      "[942] val_loss: 0.638\n",
      "[943] train_loss: 0.533\n",
      "[943] val_loss: 0.638\n",
      "[944] train_loss: 0.542\n",
      "[944] val_loss: 0.638\n",
      "[945] train_loss: 0.543\n",
      "[945] val_loss: 0.638\n",
      "[946] train_loss: 0.531\n",
      "[946] val_loss: 0.638\n",
      "[947] train_loss: 0.532\n",
      "[947] val_loss: 0.638\n",
      "[948] train_loss: 0.554\n",
      "[948] val_loss: 0.638\n",
      "[949] train_loss: 0.538\n",
      "[949] val_loss: 0.638\n",
      "[950] train_loss: 0.537\n",
      "[950] val_loss: 0.638\n",
      "[951] train_loss: 0.554\n",
      "[951] val_loss: 0.638\n",
      "[952] train_loss: 0.527\n",
      "[952] val_loss: 0.638\n",
      "[953] train_loss: 0.548\n",
      "[953] val_loss: 0.638\n",
      "[954] train_loss: 0.550\n",
      "[954] val_loss: 0.638\n",
      "[955] train_loss: 0.534\n",
      "[955] val_loss: 0.638\n",
      "[956] train_loss: 0.558\n",
      "[956] val_loss: 0.638\n",
      "[957] train_loss: 0.528\n",
      "[957] val_loss: 0.638\n",
      "[958] train_loss: 0.535\n",
      "[958] val_loss: 0.638\n",
      "[959] train_loss: 0.530\n",
      "[959] val_loss: 0.638\n",
      "[960] train_loss: 0.551\n",
      "[960] val_loss: 0.638\n",
      "[961] train_loss: 0.538\n",
      "[961] val_loss: 0.638\n",
      "[962] train_loss: 0.532\n",
      "[962] val_loss: 0.638\n",
      "[963] train_loss: 0.536\n",
      "[963] val_loss: 0.638\n",
      "[964] train_loss: 0.546\n",
      "[964] val_loss: 0.638\n",
      "[965] train_loss: 0.544\n",
      "[965] val_loss: 0.638\n",
      "[966] train_loss: 0.528\n",
      "[966] val_loss: 0.638\n",
      "[967] train_loss: 0.545\n",
      "[967] val_loss: 0.638\n",
      "[968] train_loss: 0.527\n",
      "[968] val_loss: 0.638\n",
      "[969] train_loss: 0.542\n",
      "[969] val_loss: 0.638\n",
      "[970] train_loss: 0.538\n",
      "[970] val_loss: 0.638\n",
      "[971] train_loss: 0.539\n",
      "[971] val_loss: 0.638\n",
      "[972] train_loss: 0.531\n",
      "[972] val_loss: 0.638\n",
      "[973] train_loss: 0.531\n",
      "[973] val_loss: 0.638\n",
      "[974] train_loss: 0.549\n",
      "[974] val_loss: 0.638\n",
      "[975] train_loss: 0.543\n",
      "[975] val_loss: 0.638\n",
      "[976] train_loss: 0.542\n",
      "[976] val_loss: 0.638\n",
      "[977] train_loss: 0.552\n",
      "[977] val_loss: 0.638\n",
      "[978] train_loss: 0.541\n",
      "[978] val_loss: 0.638\n",
      "[979] train_loss: 0.541\n",
      "[979] val_loss: 0.638\n",
      "[980] train_loss: 0.549\n",
      "[980] val_loss: 0.638\n",
      "[981] train_loss: 0.528\n",
      "[981] val_loss: 0.638\n",
      "[982] train_loss: 0.548\n",
      "[982] val_loss: 0.638\n",
      "[983] train_loss: 0.541\n",
      "[983] val_loss: 0.638\n",
      "[984] train_loss: 0.545\n",
      "[984] val_loss: 0.638\n",
      "[985] train_loss: 0.543\n",
      "[985] val_loss: 0.638\n",
      "[986] train_loss: 0.539\n",
      "[986] val_loss: 0.638\n",
      "[987] train_loss: 0.541\n",
      "[987] val_loss: 0.638\n",
      "[988] train_loss: 0.528\n",
      "[988] val_loss: 0.638\n",
      "[989] train_loss: 0.536\n",
      "[989] val_loss: 0.638\n",
      "[990] train_loss: 0.545\n",
      "[990] val_loss: 0.638\n",
      "[991] train_loss: 0.538\n",
      "[991] val_loss: 0.638\n",
      "[992] train_loss: 0.533\n",
      "[992] val_loss: 0.638\n",
      "[993] train_loss: 0.525\n",
      "[993] val_loss: 0.638\n",
      "[994] train_loss: 0.540\n",
      "[994] val_loss: 0.638\n",
      "[995] train_loss: 0.542\n",
      "[995] val_loss: 0.638\n",
      "[996] train_loss: 0.536\n",
      "[996] val_loss: 0.638\n",
      "[997] train_loss: 0.535\n",
      "[997] val_loss: 0.638\n",
      "[998] train_loss: 0.541\n",
      "[998] val_loss: 0.638\n",
      "[999] train_loss: 0.546\n",
      "[999] val_loss: 0.638\n",
      "[1000] train_loss: 0.553\n",
      "[1000] val_loss: 0.638\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ハイパーパラメータの設定\n",
    "PATH = \"/workspace/notes/metric/result/7-27/\"\n",
    "lr = 0.000001\n",
    "batch_size = 128\n",
    "epochs = 1000\n",
    "train_accuracy = 0\n",
    "mini_correct = 0\n",
    "mini_total = 0\n",
    "running_acc = 0\n",
    "\n",
    "alpha = 0.01 # 正則化パラメータ\n",
    "\n",
    "t_loss = []\n",
    "#混同行列用リスト\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "#プロットデータ格納用\n",
    "plot_outs = np.empty(shape=(1, 7))\n",
    "\n",
    "\n",
    "torch_seed()\n",
    "\n",
    "# データセットの作成\n",
    "# dataset = YourCustomDataset()\n",
    "\n",
    "# データローダーの作成\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# モデルの定義\n",
    "model = RNN(3)\n",
    "# model = CNN()\n",
    "# model = CNN_1d()\n",
    "# model = ResNet50(block, 3) #自分で定義したresnet50\n",
    "# model = ResNet101(block,3)\n",
    "# model = ResNet18(block,3)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=100, path=PATH+ '0.0001' + '_rnn_weight.pth') #val_lossが減らないと打ち止め\n",
    "model.load_state_dict(torch.load(PATH + '0.0001' + '_rnn_win200_weight.pth', map_location=device)) #再度学習する場合にはモデル読み込み\n",
    "\n",
    "model.cuda(device)\n",
    "\n",
    "\n",
    "# 損失関数の定義\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.HuberLoss(reduction='mean', delta=1.0)\n",
    "\n",
    "# オプティマイザの定義\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=alpha)\n",
    "\n",
    "\n",
    "# 学習ループ\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "#     for i, data in enumerate(dataloader):\n",
    "    for i, data in enumerate(train_loader):\n",
    "#         print(i)\n",
    "        inputs, labels, items = data\n",
    "        inputs = torch.squeeze(inputs,dim=1) #1D_CNNの場合のみ(次元削減)\n",
    "        labels = torch.squeeze(labels,dim=1) #1D_CNNの場合のみ(次元削減)\n",
    "        labels = torch.squeeze(labels,dim=1) #1D_CNNの場合のみ(次元削減)\n",
    "\n",
    "#         print(labels.shape)\n",
    "        inputs = inputs.unsqueeze(3)\n",
    "#         print(inputs.shape)\n",
    "        inputs_cuda = inputs.cuda(device)\n",
    "        labels_cuda = labels.cuda(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 順伝播\n",
    "#         inputs = torch.squeeze(inputs,dim=1) #1D_CNNの場合のみ(次元削減)\n",
    "        outputs = model(inputs_cuda)\n",
    "\n",
    "        # 誤差の計算と逆伝播\n",
    "        loss = criterion(outputs, labels_cuda)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 統計情報の表示\n",
    "        train_loss += loss.item() #1エポックでのlossの合計を出す\n",
    "\n",
    "        \n",
    "    print('[%d] train_loss: %.3f' % (epoch + 1, train_loss/len(train_loader)))\n",
    "    t_loss.append(train_loss/len(train_loader))\n",
    "#     print(\"output:\", outputs)\n",
    "#     print(\"label:\", labels_cuda)\n",
    "\n",
    "    # モデルの推論モードへの切り替え\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for inputs, labels, items in val_loader:\n",
    "            inputs = torch.squeeze(inputs,dim=1) #1D_CNNの場合のみ(次元削減)\n",
    "            labels = torch.squeeze(labels,dim=1) #1D_CNNの場合のみ(次元削減)\n",
    "            labels = torch.squeeze(labels,dim=1) #1D_CNNの場合のみ(次元削減)\n",
    "            inputs = inputs.unsqueeze(3)\n",
    "#             print(inputs.shape)\n",
    "            ev_inputs = inputs.cuda(device)\n",
    "            label_cuda = labels.cuda(device)\n",
    "            outputs = model(ev_inputs)\n",
    "            cp_out = outputs.to('cpu').detach().numpy().copy()\n",
    "            cp_item = items.to('cpu').detach().numpy().copy()\n",
    "            cp_label = labels.to('cpu').detach().numpy().copy()\n",
    "            cp_item = cp_item.reshape(-1, 1)\n",
    "#             print(outputs.shape)\n",
    "            plot_out = np.hstack((cp_out, cp_item)) #座標データとラベルをくっつける\n",
    "            plot_out = np.hstack((plot_out, cp_label)) #座標データとラベルをくっつける\n",
    "#             print(plot_out)\n",
    "            plot_outs = np.insert(plot_outs, 1, plot_out, axis = 0) #プロット用にモデル出力座標とラベルを吐き出す\n",
    "#             print(plot_outs)\n",
    "    #         print(\"output:\",ev_inputs.shape)\n",
    "    #         print(\"output:\",outputs.shape)\n",
    "\n",
    "            loss = criterion(outputs, label_cuda)\n",
    "        \n",
    "        \n",
    "             # 統計情報の表示\n",
    "            val_loss += loss.item()\n",
    "        val_losses.append(val_loss/len(val_loader))\n",
    "        print('[%d] val_loss: %.3f' % (epoch + 1, val_loss/len(val_loader)))\n",
    "        \n",
    "#     early_stopping(val_loss/len(val_loader), model)\n",
    "#     if early_stopping.early_stop:\n",
    "#         print(\"Early Stopping\")\n",
    "#         break # 打ち切り\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "# print(\"output:\", outputs)\n",
    "# print(\"label:\", labels_cuda)\n",
    "torch.save(model.state_dict(), PATH+ '0.000001' + '_rnn_win200_weight.pth')\n",
    "print('Finished Training')\n",
    "slack.notify(text=\"metric-learning rnn classify layer:\"  + \"の学習が終了しました\" + slack_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ea7ce3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sinbun 0\n",
      "358\n",
      "[1] test_loss: 0.087\n",
      "[2] test_loss: 0.183\n",
      "[3] test_loss: 0.283\n",
      "[4] test_loss: 0.376\n",
      "[5] test_loss: 0.465\n",
      "[6] test_loss: 0.561\n"
     ]
    }
   ],
   "source": [
    "#未学習のデータを入れる\n",
    "test_dataset = CustomDataset(root_dir='/workspace/notes/metric/data/test/',transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# モデルの推論モードへの切り替え\n",
    "model.eval()\n",
    "# model.to('cpu')\n",
    "\n",
    "# 推論用データセットの作成\n",
    "# test_loader = val_loader\n",
    "\n",
    "#混同行列用リスト\n",
    "test_losses = []\n",
    "\n",
    "loop = 0\n",
    "\n",
    "#プロットデータ格納用\n",
    "test_outs = np.empty(shape=(1, 7))\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, labels, items in test_loader:\n",
    "        inputs = torch.squeeze(inputs,dim=1) #1D_CNNの場合のみ(次元削減)\n",
    "        labels = torch.squeeze(labels,dim=1) #1D_CNNの場合のみ(次元削減)\n",
    "        labels = torch.squeeze(labels,dim=1) #1D_CNNの場合のみ(次元削減)\n",
    "        inputs = inputs.unsqueeze(3)\n",
    "        ev_inputs = inputs.cuda(device)\n",
    "        label_cuda = labels.cuda(device)\n",
    "        outputs = model(ev_inputs)\n",
    "        cp_out = outputs.to('cpu').detach().numpy().copy()\n",
    "        cp_item = items.to('cpu').detach().numpy().copy()\n",
    "        cp_label = labels.to('cpu').detach().numpy().copy()\n",
    "        cp_item = cp_item.reshape(-1, 1)\n",
    "#         print(cp_item)\n",
    "#         print(cp_label.shape)\n",
    "        test_out = np.hstack((cp_out, cp_item)) #座標データとラベルをくっつける\n",
    "        test_out = np.hstack((test_out, cp_label)) #座標データとラベルをくっつける\n",
    "#             print(plot_out)\n",
    "        test_outs = np.insert(test_outs, 1, plot_out, axis = 0) #プロット用にモデル出力座標とラベルを吐き出す\n",
    "#             print(plot_outs)\n",
    "#         print(\"output:\",ev_inputs.shape)\n",
    "#         print(\"output:\",outputs.shape)\n",
    "\n",
    "        loss = criterion(outputs, label_cuda)\n",
    "\n",
    "\n",
    "         # 統計情報の表示\n",
    "        test_loss += loss.item()\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "        print('[%d] test_loss: %.3f' % (loop + 1, test_loss/len(test_loader)))\n",
    "        loop+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5822de5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(range(len(t_loss)), t_loss, label='train_loss')\n",
    "plt.plot(range(len(val_losses)), val_losses, label='val_loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('loss')\n",
    "# plt.ylim(0, 0.65)\n",
    "plt.grid()\n",
    "# plt.savefig(\"/workspace/notes/metric/result/7-27/rnn_win200_lr10e-6.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0dd38e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#val_dataの三次元プロット\n",
    "\n",
    "# gomu 0\n",
    "# colk 1\n",
    "# wood_hinoki 2\n",
    "# arumi 3\n",
    "# wasi 4\n",
    "# buta-ura 5\n",
    "\n",
    "name_list = dataset.__classlist__()\n",
    "# print(name_list[0])\n",
    "\n",
    "import matplotlib.cm as cm \n",
    "colors = cm.rainbow(np.linspace(0, 1, 8))\n",
    "\n",
    "# print(plot_outs[0, :]) #emptyの代償、これを抜く\n",
    "plot_outs = np.delete(plot_outs, 0, 0)\n",
    "# print(plot_outs[6])\n",
    "# test_outs = np.delete(test_outs, 0, 0)\n",
    "# print(plot_outs[0, :]) #emptyの代償、これを抜く\n",
    "# print(plot_outs[:, 3])\n",
    "\n",
    "\n",
    "#素材ラベルごとにデータ分割\n",
    "plt.figure(figsize=(6, 6))\n",
    "rabel_0 = plot_outs[np.any(plot_outs == 0, axis=1), :] \n",
    "rabel_1 = plot_outs[np.any(plot_outs == 1, axis=1), :] \n",
    "rabel_2 = plot_outs[np.any(plot_outs == 2, axis=1), :] \n",
    "rabel_3 = plot_outs[np.any(plot_outs == 3, axis=1), :] \n",
    "rabel_4 = plot_outs[np.any(plot_outs == 4, axis=1), :] \n",
    "rabel_5 = plot_outs[np.any(plot_outs == 5, axis=1), :] \n",
    "# new = plot_outs[:, :] #new_data sinbun\n",
    "# print(new.shape)\n",
    "\n",
    "\n",
    "#valデータ\n",
    "dim1 = 0\n",
    "dim2 = 1\n",
    "data_skip_num = 100 #どんだけデータを表示するか\n",
    "\n",
    "plt.scatter(rabel_0[::data_skip_num, dim1], rabel_0[::data_skip_num, dim2], marker = \".\", color='red', label= name_list[0]) \n",
    "plt.scatter(rabel_1[::data_skip_num, dim1], rabel_1[::data_skip_num, dim2], marker = \".\", color='blue', label= name_list[1])\n",
    "plt.scatter(rabel_2[::data_skip_num, dim1], rabel_2[::data_skip_num, dim2], marker = \".\", color='gold', label= name_list[2])\n",
    "plt.scatter(rabel_3[::data_skip_num, dim1], rabel_3[::data_skip_num, dim2], marker = \".\", color='green', label= name_list[3])\n",
    "plt.scatter(rabel_4[::data_skip_num, dim1], rabel_4[::data_skip_num, dim2], marker = \".\", color='darkviolet', label= name_list[4])\n",
    "plt.scatter(rabel_5[::data_skip_num, dim1], rabel_5[::data_skip_num, dim2], marker = \".\", color='pink', label= name_list[5])\n",
    "# plt.scatter(new[::data_skip_num, dim1], new[::data_skip_num, dim2], marker = \".\", color='orange', label='sinbun')\n",
    "\n",
    "\n",
    "\n",
    "#心理実験のデータ\n",
    "sin_dim1 = 4\n",
    "sin_dim2 = 5\n",
    "marker_size = 200\n",
    "plt.scatter(rabel_0[0, sin_dim1], rabel_0[0, sin_dim2], s=marker_size, marker = \".\", label='sin-'+ name_list[0], color='red', edgecolors='black') \n",
    "plt.scatter(rabel_1[0, sin_dim1], rabel_1[0, sin_dim2], s=marker_size, marker = \".\", label='sin-'+ name_list[1], color='blue', edgecolors='black')\n",
    "plt.scatter(rabel_2[0, sin_dim1], rabel_2[0, sin_dim2], s=marker_size, marker = \".\", label='sin-'+ name_list[2], color='gold', edgecolors='black')\n",
    "plt.scatter(rabel_3[0, sin_dim1], rabel_3[0, sin_dim2], s=marker_size, marker = \".\", label='sin-'+ name_list[3], color='green', edgecolors='black')\n",
    "plt.scatter(rabel_4[0, sin_dim1], rabel_4[0, sin_dim2], s=marker_size, marker = \".\", label='sin-'+ name_list[4], color='darkviolet', edgecolors='black')\n",
    "plt.scatter(rabel_4[0, sin_dim1], rabel_4[0, sin_dim2], s=marker_size, marker = \".\", label='sin-'+ name_list[5], color='pink', edgecolors='black')\n",
    "# plt.scatter(new[0, sin_dim1], new[0, sin_dim2], s=marker_size, marker = \".\", label='sin-sinbun', edgecolors='black')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.xlabel(\"dim1\")\n",
    "plt.ylabel(\"dim2\")\n",
    "# plt.ylim(-1.3, -0.75)\n",
    "# plt.savefig(\"/workspace/notes/metric/result/7-14/figure/rnn/rnn_win2000_lr10e-6_dim_1-2.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b331ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim2 = 1\n",
    "dim3 = 2\n",
    "data_skip_num = 100 #どんだけデータを表示するか\n",
    "plt.scatter(rabel_0[::data_skip_num, dim2], rabel_0[::data_skip_num, dim3], marker = \".\", color='red', label= name_list[0]) \n",
    "plt.scatter(rabel_1[::data_skip_num, dim2], rabel_1[::data_skip_num, dim3], marker = \".\", color='blue', label= name_list[1])\n",
    "plt.scatter(rabel_2[::data_skip_num, dim2], rabel_2[::data_skip_num, dim3], marker = \".\", color='gold', label= name_list[2])\n",
    "plt.scatter(rabel_3[::data_skip_num, dim2], rabel_3[::data_skip_num, dim3], marker = \".\", color='green', label= name_list[3])\n",
    "plt.scatter(rabel_4[::data_skip_num, dim2], rabel_4[::data_skip_num, dim3], marker = \".\", color='darkviolet', label= name_list[4])\n",
    "plt.scatter(rabel_5[::data_skip_num, dim2], rabel_5[::data_skip_num, dim3], marker = \".\", color='pink', label= name_list[5])\n",
    "# plt.scatter(new[::data_skip_num, dim2], new[::data_skip_num, dim3], marker = \".\", color='orange', label='sinbun')\n",
    "\n",
    "\n",
    "#心理実験のデータ\n",
    "sin_dim2 = 5\n",
    "sin_dim3 = 6\n",
    "marker_size = 200\n",
    "plt.scatter(rabel_0[0, sin_dim2], rabel_0[0, sin_dim3], s=marker_size, marker = \".\", color='red', label='sin-'+ name_list[0], edgecolors='black') \n",
    "plt.scatter(rabel_1[0, sin_dim2], rabel_1[0, sin_dim3], s=marker_size, marker = \".\", color='blue', label='sin-'+ name_list[1], edgecolors='black')\n",
    "plt.scatter(rabel_2[0, sin_dim2], rabel_2[0, sin_dim3], s=marker_size, marker = \".\", color='gold', label='sin-'+ name_list[2], edgecolors='black')\n",
    "plt.scatter(rabel_3[0, sin_dim2], rabel_3[0, sin_dim3], s=marker_size, marker = \".\", color='green', label='sin-'+ name_list[3], edgecolors='black')\n",
    "plt.scatter(rabel_4[0, sin_dim2], rabel_4[0, sin_dim3], s=marker_size, marker = \".\", color='darkviolet', label='sin-'+ name_list[4], edgecolors='black')\n",
    "plt.scatter(rabel_5[0, sin_dim2], rabel_5[0, sin_dim3], s=marker_size, marker = \".\", color='pink', label='sin-'+ name_list[5], edgecolors='black')\n",
    "# plt.scatter(new[0, sin_dim2], new[0, sin_dim3], s=marker_size, marker = \".\", color='orange', label='sin-sinbun', edgecolors='black')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.0, 1), borderaxespad=0, loc='upper right', fontsize=8)\n",
    "plt.grid()\n",
    "plt.xlabel(\"dim2\")\n",
    "plt.ylabel(\"dim3\")\n",
    "# plt.savefig(\"/workspace/notes/metric/result/7-14/figure/rnn/rnn_win2048_lr10e-6_dim_2-3.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "471e7867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#valデータ\n",
    "dim1 = 0\n",
    "dim3 = 2\n",
    "data_skip_num = 100 #どんだけデータを表示するか\n",
    "# plt.scatter(rabel_0[::500, 0], rabel_0[::500, 2], marker = \"x\", label='buta-ura') \n",
    "# plt.scatter(rabel_1[::500, 0], rabel_1[::500, 2], marker = \"+\", label='wood')\n",
    "# plt.scatter(rabel_2[::500, 0], rabel_2[::500, 2], marker = \"v\", label='wasi')\n",
    "# plt.scatter(rabel_3[::500, 0], rabel_3[::500, 2], marker = \".\", label='colk')\n",
    "# plt.scatter(rabel_4[::500, 0], rabel_4[::500, 2], marker = \"*\", label='gomu')\n",
    "plt.scatter(rabel_0[::data_skip_num, dim1], rabel_0[::data_skip_num, dim3], marker = \".\", color='red', label= name_list[0]) \n",
    "plt.scatter(rabel_1[::data_skip_num, dim1], rabel_1[::data_skip_num, dim3], marker = \".\", color='blue', label= name_list[1])\n",
    "plt.scatter(rabel_2[::data_skip_num, dim1], rabel_2[::data_skip_num, dim3], marker = \".\", color='gold', label= name_list[2])\n",
    "plt.scatter(rabel_3[::data_skip_num, dim1], rabel_3[::data_skip_num, dim3], marker = \".\", color='green', label= name_list[3])\n",
    "plt.scatter(rabel_4[::data_skip_num, dim1], rabel_4[::data_skip_num, dim3], marker = \".\", color='darkviolet', label= name_list[4])\n",
    "plt.scatter(rabel_5[::data_skip_num, dim1], rabel_5[::data_skip_num, dim3], marker = \".\", color='pink', label= name_list[5])\n",
    "# plt.scatter(new[::data_skip_num, dim1], new[::data_skip_num, dim3], marker = \".\", color='orange', label='sinbun')\n",
    "\n",
    "\n",
    "#心理実験のデータ\n",
    "sin_dim1 = 4\n",
    "sin_dim3 = 6\n",
    "marker_size = 200\n",
    "\n",
    "plt.scatter(rabel_0[0, sin_dim1], rabel_0[0, sin_dim3], s=marker_size, marker = \".\", color='red', label='sin-'+ name_list[0], edgecolors='black') \n",
    "plt.scatter(rabel_1[0, sin_dim1], rabel_1[0, sin_dim3], s=marker_size, marker = \".\", color='blue', label='sin-'+ name_list[1], edgecolors='black')\n",
    "plt.scatter(rabel_2[0, sin_dim1], rabel_2[0, sin_dim3], s=marker_size, marker = \".\", color='gold', label='sin-'+ name_list[2], edgecolors='black')\n",
    "plt.scatter(rabel_3[0, sin_dim1], rabel_3[0, sin_dim3], s=marker_size, marker = \".\", color='green', label='sin-'+ name_list[3], edgecolors='black')\n",
    "plt.scatter(rabel_4[0, sin_dim1], rabel_4[0, sin_dim3], s=marker_size, marker = \".\", color='darkviolet', label='sin-'+ name_list[4], edgecolors='black')\n",
    "plt.scatter(rabel_5[0, sin_dim1], rabel_5[0, sin_dim3], s=marker_size, marker = \".\", color='pink', label='sin-'+ name_list[5], edgecolors='black')\n",
    "# plt.scatter(new[0, sin_dim1], new[0, sin_dim3], s=marker_size, marker = \".\", color='orange', label='sin-sinbun', edgecolors='black')\n",
    "\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.0, 1), borderaxespad=0, loc='upper right', fontsize=8)\n",
    "plt.grid()\n",
    "plt.xlabel(\"dim1\")\n",
    "plt.ylabel(\"dim3\")\n",
    "# plt.savefig(\"/workspace/notes/metric/result/7-14/figure/rnn/rnn_win2048_lr10e-6_dim_1-3.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8321b705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.105\n"
     ]
    }
   ],
   "source": [
    "# モデルの推論モードへの切り替え\n",
    "model.eval()\n",
    "# model.to('cpu')\n",
    "\n",
    "#混同行列用リスト\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = torch.squeeze(inputs,dim=1) #1D_CNNの場合のみ(次元削減)\n",
    "        labels = torch.squeeze(labels,dim=1) #1D_CNNの場合のみ(次元削減)\n",
    "        labels = torch.squeeze(labels,dim=1) #1D_CNNの場合のみ(次元削減)\n",
    "        inputs = inputs.unsqueeze(3)\n",
    "        ev_inputs = inputs.cuda(device)\n",
    "        label_cuda = labels.cuda(device)\n",
    "#         inputs = torch.squeeze(inputs,dim=1)\n",
    "#         ev_inputs = inputs #1D_CNNの場合のみ(次元削減)\n",
    "        outputs = model(ev_inputs)\n",
    "#         print(\"output:\",ev_inputs.shape)\n",
    "#         print(\"output:\",outputs.shape)\n",
    "\n",
    "        loss = criterion(outputs, label_cuda)\n",
    "         # 統計情報の表示\n",
    "        test_loss += loss.item()\n",
    "        test_losses.append(test_loss/len(val_loader))\n",
    "\n",
    "    \n",
    "print('loss: %.3f' % (test_loss/len(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "15311362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(12).reshape((3, 4))\n",
    "\n",
    "print(a[np.all(a[:, 2] > 3, axis=0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a14932b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip -q /workspace/notes/metric/6-20-20230620T113347Z-001.zip -d /workspace/notes/metric/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f8d069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
